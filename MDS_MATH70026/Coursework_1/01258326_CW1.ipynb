{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Data Science - Coursework 1\n",
    "## Amaury Francou - CID : 01258326\n",
    "### amaury.francou16@imperial.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xHSx8l1tUYN"
   },
   "source": [
    "<a name=\"outline\"></a>\n",
    "## Outline\n",
    "\n",
    "- [Task 1](#task-1): Regression\n",
    "  - [1.1](#q11) Linear regression\n",
    "  - [1.2](#q12) Ridge regression\n",
    "  - [1.3](#q13) Relaxation of Lasso regression\n",
    "- [Task 2](#task-2): Classification\n",
    "  - [2.1](#q21) kNN classifier\n",
    "  - [2.2](#q22) Random forest \n",
    "  - [2.3](#q23) Support vector machine (SVM) \n",
    "- [Task 3](#task-3): Mastery component \n",
    "  - [3.1](#q31) Logistic regression and bagging \n",
    "  - [3.2](#q32) Kernelised SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from numpy import random\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3sSxTA3yzhN"
   },
   "source": [
    "<a name=\"task-1\"></a>\n",
    "# Task 1: Regression [^](#outline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gY8lFgnyzsF"
   },
   "source": [
    "<a name=\"q11\"></a>\n",
    "\n",
    "## 1.1 Linear regression  [^](#outline)\n",
    "\n",
    "We here implement a linear regression model to predict the toxicity factor (LC50) target variable, using all the other features present in the $\\texttt{chemistry_samples.csv}$ file dataset as predictors.\n",
    "\n",
    "We start by visualizing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1_Dz(Z)</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>NdssC</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>FV1</th>\n",
       "      <th>VFV</th>\n",
       "      <th>FV2</th>\n",
       "      <th>FV3</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.661280</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>1.602232</td>\n",
       "      <td>1.994272</td>\n",
       "      <td>0.836488</td>\n",
       "      <td>3.153623</td>\n",
       "      <td>15.893033</td>\n",
       "      <td>-27.724370</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.756698</td>\n",
       "      <td>5.506249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.936362</td>\n",
       "      <td>1.154287</td>\n",
       "      <td>1.146997</td>\n",
       "      <td>0.904295</td>\n",
       "      <td>2.948308</td>\n",
       "      <td>5.141095</td>\n",
       "      <td>13.590177</td>\n",
       "      <td>-31.821521</td>\n",
       "      <td>-13.408855</td>\n",
       "      <td>1.161298</td>\n",
       "      <td>6.636791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964144</td>\n",
       "      <td>0.415485</td>\n",
       "      <td>1.481028</td>\n",
       "      <td>2.136585</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>-1.156783</td>\n",
       "      <td>15.989419</td>\n",
       "      <td>-3.699312</td>\n",
       "      <td>2.561525</td>\n",
       "      <td>0.500115</td>\n",
       "      <td>1.563388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.074617</td>\n",
       "      <td>1.417296</td>\n",
       "      <td>0.486216</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.066980</td>\n",
       "      <td>2.610960</td>\n",
       "      <td>7.962046</td>\n",
       "      <td>-16.374439</td>\n",
       "      <td>2.448975</td>\n",
       "      <td>1.481888</td>\n",
       "      <td>6.248432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448569</td>\n",
       "      <td>0.836892</td>\n",
       "      <td>1.951012</td>\n",
       "      <td>0.028318</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>1.851095</td>\n",
       "      <td>22.285266</td>\n",
       "      <td>-9.526361</td>\n",
       "      <td>2.870400</td>\n",
       "      <td>0.649234</td>\n",
       "      <td>3.676796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIC0  SM1_Dz(Z)    GATS1i     NdsCH     NdssC     MLOGP        FV1  \\\n",
       "0  3.661280   0.658363  1.602232  1.994272  0.836488  3.153623  15.893033   \n",
       "1  3.936362   1.154287  1.146997  0.904295  2.948308  5.141095  13.590177   \n",
       "2  0.964144   0.415485  1.481028  2.136585  0.043679 -1.156783  15.989419   \n",
       "3  2.074617   1.417296  0.486216  0.000908 -0.066980  2.610960   7.962046   \n",
       "4  1.448569   0.836892  1.951012  0.028318 -0.039121  1.851095  22.285266   \n",
       "\n",
       "         VFV        FV2       FV3      LC50  \n",
       "0 -27.724370   0.059355  0.756698  5.506249  \n",
       "1 -31.821521 -13.408855  1.161298  6.636791  \n",
       "2  -3.699312   2.561525  0.500115  1.563388  \n",
       "3 -16.374439   2.448975  1.481888  6.248432  \n",
       "4  -9.526361   2.870400  0.649234  3.676796  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load chemistry data into a pandas dataframe \n",
    "df_chemistry = pd.read_csv(\"chemistry_samples.csv\")\n",
    "# Displaying the first five rows\n",
    "df_chemistry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 4111\n"
     ]
    }
   ],
   "source": [
    "# Finding number of samples\n",
    "nb_samples = df_chemistry['CIC0'].count()\n",
    "print('Number of samples : ' + str(nb_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Training\n",
    "\n",
    "We will consider that this given data has been cleaned and pre-processed appropriately, and is further ready for training. The first 10 columns give the 10 predictors and the 11th last column gives the corresponding target variable.\n",
    "\n",
    "We here consider a regression problem, in the sense of predicting a continuous output (i.e. a quantitative value, by opposition to a categorical output). The loss function that we use in this linear regression is the $\\textit{mean squared error}$.\n",
    "\n",
    "We consider a model $f_{LS}(\\mathbf{x}^{(i)},\\mathbf{\\beta}) = \\beta_0 + \\beta_1 x_1^{(i)} + \\dots + \\beta_{10} x_{10}^{(i)}$ where $\\mathbf{\\beta}$ is an appropriate parameter to be found and the $\\mathbf{x}^{(i)}$ are our predictors.\n",
    "\n",
    "We define an augmented matrix $\\mathbf{X}$ containing the descriptor variables in the same way as in the lecture notes. As well, we define a vector $\\mathbf{y}$ containing the observed target variables. The solution of the Normal Equation reads : $\\mathbf{\\beta}^{\\star} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our dataframe to a numpy array\n",
    "dataArray = df_chemistry.to_numpy()\n",
    "predictorsArray = df_chemistry.to_numpy()[:,:10]\n",
    "targetArray = df_chemistry.to_numpy()[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a function that retreives the minimizing beta given an array of predictors and a target value vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBeta(predictors = predictorsArray, target = targetArray) :\n",
    "    \"\"\"\n",
    "    This function computes a beta vector minimizing the mean squared error upon a linear model, \n",
    "    using the closed form solution of the Normal Equation. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    # Building appropriate matrix X\n",
    "    X = np.zeros(shape = (N,p+1)) \n",
    "    X[:,0] = np.ones(N)\n",
    "    X[:,1:] = predictors\n",
    "    \n",
    "    # Computing beta \n",
    "    beta = (np.linalg.inv(X.T@X)@X.T)@target\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further retreive the corresponding beta vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [ 2.61638041e+00  4.47138333e-02  1.25871884e+00 -3.80092766e-02\n",
      "  3.63073448e-01  4.66534885e-03  3.90510052e-01 -7.46028629e-02\n",
      " -3.57069460e-02 -1.52588258e-02 -1.80315940e-03]\n"
     ]
    }
   ],
   "source": [
    "beta = getBeta()\n",
    "print('beta =', beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze the quality of our model using the coefficient of determination $R^2 = 1 - \\frac{\\sum (\\hat{y}_i - y_i)^2}{\\sum (y_i - \\bar{y})^2}$, where $y_i$ is the target value, $\\hat{y}_i$ is the predicted value and $\\bar{y}$ is the mean of all target values. \n",
    "\n",
    "We implement a function that computes this coefficient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRsquared(predictors = predictorsArray, target = targetArray, beta = beta, noIntercept = False) :\n",
    "    \"\"\"\n",
    "    This function computes the coefficient of determination of the linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    noIntercept : a boolean value - set to True if we use a model in which beta_0 = 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Rsquared : a float number - the coefficient of determination\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape\n",
    "    meanTarget = np.mean(target) # Computing the mean of the target values\n",
    "    \n",
    "    numerator = 0 # Initializing for the count\n",
    "    denominator = 0\n",
    "    \n",
    "    for i in range(N) : # Computing predictions and building R^2 fraction components\n",
    "        \n",
    "        if noIntercept : # No need for augmented vector \n",
    "            \n",
    "            predictedValue = np.dot(beta,predictors[i])\n",
    "\n",
    "        else :\n",
    "            # Building augmented x_i\n",
    "            augmentedPredictor = np.zeros(p+1)\n",
    "            augmentedPredictor[0] = 1\n",
    "            augmentedPredictor[1:] = predictors[i]\n",
    "\n",
    "            # Using our linear model y_i = beta^Tx_i\n",
    "            predictedValue = np.dot(beta,augmentedPredictor)\n",
    "        \n",
    "        # Building R^2 fraction components\n",
    "        numerator += (target[i] - predictedValue)**2\n",
    "        denominator += (target[i] - meanTarget)**2\n",
    "        \n",
    "    return 1 - (numerator/denominator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.8718259975718018\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 on the training set \n",
    "RsquaredTraining = getRsquared()\n",
    "print('R^2 =', RsquaredTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Testing\n",
    "\n",
    "We now apply our model $f_{LS}$ to the pre-cleaned testing data that has not been involved in the training phase. The aim is to predict the target variable LC50 of this testing set using the 10 same predictors. We compute the $R^2$ score on this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1_Dz(Z)</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>NdssC</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>FV1</th>\n",
       "      <th>VFV</th>\n",
       "      <th>FV2</th>\n",
       "      <th>FV3</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.617579</td>\n",
       "      <td>0.376167</td>\n",
       "      <td>1.224281</td>\n",
       "      <td>0.849464</td>\n",
       "      <td>1.101738</td>\n",
       "      <td>-0.448372</td>\n",
       "      <td>14.913614</td>\n",
       "      <td>-9.091450</td>\n",
       "      <td>-1.953849</td>\n",
       "      <td>0.328298</td>\n",
       "      <td>1.791786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.278766</td>\n",
       "      <td>0.514788</td>\n",
       "      <td>1.259734</td>\n",
       "      <td>0.210436</td>\n",
       "      <td>0.819626</td>\n",
       "      <td>4.446118</td>\n",
       "      <td>12.904817</td>\n",
       "      <td>-37.986185</td>\n",
       "      <td>-2.804426</td>\n",
       "      <td>0.452758</td>\n",
       "      <td>6.125609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.578652</td>\n",
       "      <td>0.221018</td>\n",
       "      <td>1.552583</td>\n",
       "      <td>1.007153</td>\n",
       "      <td>-0.013073</td>\n",
       "      <td>1.960720</td>\n",
       "      <td>17.393050</td>\n",
       "      <td>-27.188863</td>\n",
       "      <td>3.565159</td>\n",
       "      <td>0.341665</td>\n",
       "      <td>3.953270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.021762</td>\n",
       "      <td>1.602774</td>\n",
       "      <td>1.044233</td>\n",
       "      <td>0.054776</td>\n",
       "      <td>2.060890</td>\n",
       "      <td>4.510903</td>\n",
       "      <td>12.777434</td>\n",
       "      <td>-22.710306</td>\n",
       "      <td>-7.966119</td>\n",
       "      <td>1.729511</td>\n",
       "      <td>6.995314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.487163</td>\n",
       "      <td>0.799948</td>\n",
       "      <td>1.005727</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.146542</td>\n",
       "      <td>2.298082</td>\n",
       "      <td>13.336721</td>\n",
       "      <td>-16.839870</td>\n",
       "      <td>2.607198</td>\n",
       "      <td>0.904353</td>\n",
       "      <td>5.253633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIC0  SM1_Dz(Z)    GATS1i     NdsCH     NdssC     MLOGP        FV1  \\\n",
       "0  1.617579   0.376167  1.224281  0.849464  1.101738 -0.448372  14.913614   \n",
       "1  4.278766   0.514788  1.259734  0.210436  0.819626  4.446118  12.904817   \n",
       "2  3.578652   0.221018  1.552583  1.007153 -0.013073  1.960720  17.393050   \n",
       "3  3.021762   1.602774  1.044233  0.054776  2.060890  4.510903  12.777434   \n",
       "4  2.487163   0.799948  1.005727  0.094923  0.146542  2.298082  13.336721   \n",
       "\n",
       "         VFV       FV2       FV3      LC50  \n",
       "0  -9.091450 -1.953849  0.328298  1.791786  \n",
       "1 -37.986185 -2.804426  0.452758  6.125609  \n",
       "2 -27.188863  3.565159  0.341665  3.953270  \n",
       "3 -22.710306 -7.966119  1.729511  6.995314  \n",
       "4 -16.839870  2.607198  0.904353  5.253633  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing data into a pandas dataframe \n",
    "df_chemistry_testing = pd.read_csv(\"chemistry_test.csv\")\n",
    "# Displaying the first five rows\n",
    "df_chemistry_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 1028\n"
     ]
    }
   ],
   "source": [
    "# Finding number of samples\n",
    "nb_samples = df_chemistry_testing['CIC0'].count()\n",
    "print('Number of samples : ' + str(nb_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our dataframe to a numpy array\n",
    "testingDataArray = df_chemistry_testing.to_numpy()\n",
    "predictorsTestingArray = df_chemistry_testing.to_numpy()[:,:10]\n",
    "targetTestingArray = df_chemistry_testing.to_numpy()[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.8642933369927278\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 on the testing set | beta is unchanged\n",
    "RsquaredTesting = getRsquared(predictors = predictorsTestingArray, target = targetTestingArray, beta = beta) \n",
    "print('R^2 =', RsquaredTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a slightly smaller $R^2$ computed on the testing set than on the training set. This displays a good generalisability of the model although the linear correlation between the predictors and the target is rather low (86%).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpbJPhupy0Cj"
   },
   "source": [
    "<a name=\"q12\"></a>\n",
    "## 1.2 Ridge regression [^](#outline)\n",
    "\n",
    "We now aim to predict the same target value using the same set of predictors, this time using Ridge regression. We use a 5-fold cross-validation method to find the optimal penalty hyperparameter of the model.\n",
    "\n",
    "We seek to reduce the number of descriptors by inducing sparsity in our model. We perform this by changing the loss function used. The idea is to maintain some of the coefficients small, by applying small weights.\n",
    "\n",
    "The loss function involved here is : $L_{RIDGE}(\\beta) = ||\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta}||^2 + \\lambda||\\mathbf{\\beta}||^2$.\n",
    "\n",
    "The minimizing beta is $\\mathbf{\\beta}^{\\star} = (\\mathbf{X}^T\\mathbf{X} + \\lambda I)^{-1}\\mathbf{X}^T\\mathbf{y}$, where $\\lambda$ is a hyperparameter. \n",
    "\n",
    "Note that here we leave out the intercept $\\beta_0$ in the regularization part of the equation, in order to improve the shrinkage of the coefficients and avoid dominating values for said intercept in our linear model.\n",
    "\n",
    "### 1.2.1 Optimal $\\lambda$\n",
    "\n",
    "To find an optimal $\\lambda$, we use a 5-fold cross-validation process. We split our training data into 5 equal subsets. In turn, we then isolate one of the subsets and further train the model on the rest of the training data. We compute the mean squared error of the model, corresponding to predicting the target values of the subset that has been put aside. We repeat this operation for all the 5 subsets. We then compute the mean of mean squared errors, which gives us a measure to find the optimal lambda.\n",
    "\n",
    "We build relevant helper functions to implement this 5-fold cross-validation method.\n",
    "\n",
    "We first create a function that computes a minimizing beta, giving the parameter $\\lambda$ and the predictors and target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBetaRIDGE(lbda, predictors = dataArray[:,:10], target = dataArray[:,10]) :\n",
    "    \"\"\"\n",
    "    This function computes a beta vector minimizing the mean squared error upon a Ridge \n",
    "    regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : a float number - the penalty term\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    # Building appropriate matrix X\n",
    "    X = np.zeros(shape = (N,p+1)) \n",
    "    X[:,0] = np.ones(N)\n",
    "    X[:,1:] = predictors\n",
    "    \n",
    "    ### We now leave out the intercept ###\n",
    "    I = np.identity(p+1)\n",
    "    I[0,0] = 0\n",
    "    \n",
    "    # Computing beta \n",
    "    beta = (np.linalg.inv(X.T@X + lbda * I)@X.T)@target\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a function that computes the mean squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictors, target, beta) :\n",
    "    \"\"\"\n",
    "    This function computes the mean squared error of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : a float number - the mean squared error of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    S = 0 # Initializing the sum\n",
    "    \n",
    "    for i in range(N) : # Computing predictions and mean squared error\n",
    "        \n",
    "        # Building augmented x_i\n",
    "        augmentedPredictor = np.zeros(p+1)\n",
    "        augmentedPredictor[0] = 1\n",
    "        augmentedPredictor[1:] = predictors[i]\n",
    "        \n",
    "        # Using our linear model y_i = beta^Tx_i\n",
    "        predictedValue = np.dot(beta,augmentedPredictor)\n",
    "        \n",
    "        S += (predictedValue - target[i])**2\n",
    "        \n",
    "    return S/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally implement the 5-fold cross-validation process as described previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationRIDGE(dataArray, lbda) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of mean squared errors of a five-fold cross-validation process\n",
    "    for the RIDGE regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : a float number - the penalty term\n",
    "    dataArray : an Nxp-dimensionnal numpy array containing both the predictors and the target value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanMSE : a float number - the mean of mean squared errors of the five-fold cross-validation process\n",
    "    \"\"\"\n",
    "    \n",
    "    meanSquaredErrors = np.zeros(5) # Container for the mean squared errors\n",
    "    \n",
    "    # Building the 5 subsets indexes\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        # Training\n",
    "        beta = getBetaRIDGE(lbda = lbda, predictors = trainingSubset[:,:10], target = trainingSubset[:,10])\n",
    "        # MSE on validation subset\n",
    "        meanSquaredErrors[i] = MSE(predictors = validationSubset[:,:10], target = validationSubset[:,10], \\\n",
    "                                   beta = beta)\n",
    "    \n",
    "    return np.mean(meanSquaredErrors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to scan a range of hyperparameters $\\lambda$, in order to find one minimizing the mean of mean squared errors over the 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestLambdaRIDGE(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter lambda by using a five-fold cross-validation\n",
    "    method for the RIDGE regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nxp-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of mean squared errors and the tested lambdas \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_lambda : a float number - the optimal hyperparameter\n",
    "    lambdas : an M-dimensionnal numpy array containing the tested lambdas\n",
    "    lbda_MMSE : an M-dimensionnal numpy array containing the means of MSEs\n",
    "    best_lambda_ind : an integer - the optimal lambda index in lambdas\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    lambdas = np.linspace(start = 1e-15, stop = 1.45, num = 500) # Scanning lambdas\n",
    "    lbda_MMSE = np.zeros(lambdas.shape[0])\n",
    "    \n",
    "    for i, lbda in enumerate(lambdas) :\n",
    "        # Minimizing mean of MSE\n",
    "        lbda_MMSE[i] = fiveFoldValidationRIDGE(dataArray = dataArray, lbda = lbda)\n",
    "        if i%50 == 0 : # Prints from time to time\n",
    "            print('Cross-validation for lambda = ' + str(lbda) + ' gives a mean of MSE = ' + str(lbda_MMSE[i]))\n",
    "        \n",
    "    best_lambda_ind = np.argmin(lbda_MMSE) # A minimizer of means of MSEs\n",
    "    \n",
    "    if returnArray :\n",
    "        return lambdas[best_lambda_ind], lambdas, lbda_MMSE, best_lambda_ind\n",
    "    \n",
    "    return lambdas[best_lambda_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for lambda = 1e-15 gives a mean of MSE = 0.18792876597652347\n",
      "Cross-validation for lambda = 0.14529058116232554 gives a mean of MSE = 0.18792795031638843\n",
      "Cross-validation for lambda = 0.2905811623246501 gives a mean of MSE = 0.18792732584088223\n",
      "Cross-validation for lambda = 0.4358717434869746 gives a mean of MSE = 0.18792689014630146\n",
      "Cross-validation for lambda = 0.5811623246492992 gives a mean of MSE = 0.18792664086084016\n",
      "Cross-validation for lambda = 0.7264529058116237 gives a mean of MSE = 0.18792657564412468\n",
      "Cross-validation for lambda = 0.8717434869739482 gives a mean of MSE = 0.18792669218675254\n",
      "Cross-validation for lambda = 1.0170340681362728 gives a mean of MSE = 0.18792698820982445\n",
      "Cross-validation for lambda = 1.1623246492985975 gives a mean of MSE = 0.18792746146449585\n",
      "Cross-validation for lambda = 1.3076152304609219 gives a mean of MSE = 0.18792810973155943\n"
     ]
    }
   ],
   "source": [
    "# Computing optimal lambda\n",
    "bestLambdaRIDGE, lambdas, lbda_MMSE,\\\n",
    "best_lambda_ind = chooseBestLambdaRIDGE(dataArray = dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute this optimal $\\lambda$ and report the corresponding mean of mean squared errors : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal λ = 0.7061122244488982\n",
      "Minimum mean of MSEs = 0.18792657378004998\n"
     ]
    }
   ],
   "source": [
    "# Optimal lambda hyperparameter RIDGE\n",
    "print('Optimal λ = ' + str(bestLambdaRIDGE))\n",
    "# Corresponding mean of MSEs\n",
    "print('Minimum mean of MSEs = ' + str(lbda_MMSE[best_lambda_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9ce8a6250>]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RIDGE : Plotting mean of MSEs against different lambdas\n",
    "plt.figure()\n",
    "title = 'Mean of mean squared errors computed using five-fold cross-validation against hyperparameter $\\lambda$ \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Mean of MSEs')\n",
    "plt.plot(lambdas,lbda_MMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimal $\\lambda \\approx 0.70$ well minimizes the mean of MSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method used is a regularization method where high components in $\\beta$ are penalized. It is intended to reduce overfitting by limiting the importance of unrelevant and covariates inputs. The $\\lambda$ term controls the magnitude of this penalty. \n",
    "\n",
    "For $\\lambda = 0$ we retreive the linear regression. As $\\lambda$ grows, the coefficients of $\\beta$ will shrink accordingly, reducing the impact of unrelevant entry variables on the model, further lowering the variance but increasing the bias. This method exchanges bias against variance. A tradeoff $\\lambda$ is found using our cross-validation process.\n",
    "\n",
    "### 1.2.2 Comparing $R^2$\n",
    "\n",
    "We now use our optimal $\\lambda$ to compute the vector $\\beta$ on the entire data set. We further compute the related coefficient of determination $R^2$. We compare it with the corresponding score computed on the unused testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_RIDGE =  [ 2.61821445  0.04433401  1.25230457 -0.03780147  0.36279609  0.00495367\n",
      "  0.39065473 -0.07463672 -0.03570967 -0.01524778  0.00319929]\n"
     ]
    }
   ],
   "source": [
    "# Training on full dataset - RIDGE Regression\n",
    "betaRIDGE = getBetaRIDGE(lbda = bestLambdaRIDGE, predictors = dataArray[:,:10], target = dataArray[:,10]) \n",
    "print('beta_RIDGE = ',betaRIDGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 with RIDGE on full data set =  0.8718250436593374\n",
      "Previous R^2 = 0.8718259975718018\n",
      "Difference = -9.53912464352058e-07\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 with best lambda and on full data set\n",
    "RsquaredTrainingRIDGE = getRsquared(predictors = dataArray[:,:10], target = dataArray[:,10], \\\n",
    "                                    beta = betaRIDGE)\n",
    "print('R^2 with RIDGE on full data set = ',RsquaredTrainingRIDGE)\n",
    "print('Previous R^2 =', RsquaredTraining)\n",
    "print('Difference =', RsquaredTrainingRIDGE - RsquaredTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 with RIDGE on testing set =  0.8642791908122764\n",
      "Previous R^2 = 0.8642933369927278\n",
      "Difference = -1.4146180451368728e-05\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 with best lambda and on testing data set\n",
    "RsquaredTestingRIDGE = getRsquared(predictors = testingDataArray[:,:10], target = testingDataArray[:,10],\\\n",
    "                                   beta = betaRIDGE)\n",
    "print('R^2 with RIDGE on testing set = ',RsquaredTestingRIDGE)\n",
    "print('Previous R^2 =', RsquaredTesting)\n",
    "print('Difference =', RsquaredTestingRIDGE - RsquaredTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear correlation is moderately worse using ridge regression, as there is a difference of order $10^{-6}$ in the coefficient of determination, both on training and testing set. The shrinkage method used - here with a small penalty term that well balances variance and bias - does not allow to improve our model's predictions. In our considered case, a linear method may not be capable of modelling the perhaps more subtil relationship that exists between our predictors and our target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9OTrzYly0Oz"
   },
   "source": [
    "<a name=\"q13\"></a>\n",
    "\n",
    "## 1.3 Relaxation of Lasso regression  [^](#outline)\n",
    "\n",
    "We now consider another shrinkage method : a relaxation of the Lasso optimisation. Namely, we now aim to minimize $L_{LASSO} = ||\\mathbf{y}-\\mathbf{X}\\beta||^2 + \\lambda \\sum_i^p L_c(\\beta_i)$, where the 1-norm is replaced by the use of smooth Huber functions $L_c$ defined by :$$L_c(\\beta)=\\begin{cases}\\frac{1}{2}\\beta^2 \\text{ if $|\\beta|\\leq c$}\\\\c(|\\beta|-\\frac{1}{2}c) \\text{ otherwise}\\end{cases}$$ This formulation makes the error function differentiable, and further allows the use of the gradient descent algorithm. The parameter $c$ is here fixed.\n",
    "\n",
    "### 1.3.1 Computing the gradient\n",
    "\n",
    "In order to perform the algorithm, we compute the gradient of the objective function :$$ \\nabla L_{LASSO}(\\beta) = -2\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)+ \\lambda\\sum_i^p\\frac{dL_c(\\beta_i)}{d\\beta}\\vec{\\mathbf{e}}_i \\text{ with - as long as $c>0$ - }\\frac{dL_c(\\beta_i)}{d\\beta}=\\begin{cases}\\beta_i \\text{ if $|\\beta_i|\\leq c$} \\\\c\\frac{\\beta_i}{|\\beta_i|}\\text{ otherwise}\\end{cases}$$ \n",
    "\n",
    "We implement those functions and gradients in the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "jg2TWPoT0J0Z"
   },
   "outputs": [],
   "source": [
    "def huber(beta, c=1e-6) :\n",
    "    \"\"\"\n",
    "    This function computes the regularization term of the relaxed LASSO method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    c : a float number - a fixed parameter of Huber functions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    huberTerm : a float number - the regularization term\n",
    "    \"\"\"\n",
    "    \n",
    "    p = beta.shape[0] - 1 # Retreiving the number of parameters\n",
    "    Lcs = np.zeros(p+1)\n",
    "    \n",
    "    for i in range(p+1) : # Using Lc formula\n",
    "        if np.abs(beta[i]) <= c :\n",
    "            Lcs[i] = 0.5 * beta[i]**2\n",
    "        else :\n",
    "            Lcs[i] = c * (np.abs(beta[i]) - 0.5 * c)\n",
    "            \n",
    "    return np.sum(Lcs) \n",
    "\n",
    "\n",
    "\n",
    "def LASSOfunc(predictors, target, beta, lbda, c=1e-6) :\n",
    "    \"\"\"\n",
    "    This function computes the objective function of the relaxed LASSO method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    lbda : a float number - the penalty term\n",
    "    c : a float number - a fixed parameter of Huber functions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    L : a float number - the value of the objective function\n",
    "    \"\"\"\n",
    "        \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    # Building appropriate matrix X\n",
    "    X = np.zeros(shape = (N,p+1)) \n",
    "    X[:,0] = np.ones(N)\n",
    "    X[:,1:] = predictors\n",
    "    \n",
    "    # Using formula\n",
    "    L = np.linalg.norm(target - X@beta)**2 + lbda * huber(beta, c=c)\n",
    "    \n",
    "    return L\n",
    "    \n",
    "def grad_huber(beta, c=1e-6) :\n",
    "    \"\"\"\n",
    "    This function computes the gradient of the regularization term of the relaxed LASSO method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    c : a float number - a fixed parameter of Huber functions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    huberTermGradient : a p+1-dimensionnal numpy array - the regularization term gradient\n",
    "    \"\"\"\n",
    "    p = beta.shape[0] - 1 # Retreiving the number of parameters\n",
    "    dLcs = np.zeros(p+1)\n",
    "    \n",
    "    for i in range(p+1) : # Using dLc/dB formula\n",
    "        if np.abs(beta[i]) <= c :\n",
    "            dLcs[i] = beta[i]\n",
    "        else :\n",
    "            dLcs[i] = c * beta[i]/np.abs(beta[i])\n",
    "            \n",
    "    return dLcs\n",
    "\n",
    "def gradLASSO(predictors, target, beta, lbda, c=1e-6) :\n",
    "    \"\"\"\n",
    "    This function computes the gradient of the objective function of the relaxed LASSO method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "    beta : a p+1-dimensionnal numpy array containing the minimizing beta\n",
    "    lbda : a float number - the penalty term\n",
    "    c : a float number - a fixed parameter of Huber functions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : a p+1-dimensionnal numpy array - the gradient of the objective function\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    # Building appropriate matrix X\n",
    "    X = np.zeros(shape = (N,p+1)) \n",
    "    X[:,0] = np.ones(N)\n",
    "    X[:,1:] = predictors\n",
    "    \n",
    "    # Using analytical expression with scaling\n",
    "    grad = -2 * (X.T/N)@(target - X@beta) + lbda * grad_huber(beta, c=1e-6)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the gradient descent algorithm using fixed step size, to compute a vector $\\beta$ minimizing our objective function $L_{LASSO}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "U7hMrAxK0LQN"
   },
   "outputs": [],
   "source": [
    "def gradientDescentHuber(predictors, target, lbda, initialGuess = np.array([]), step = 1e-9, maxIt = 100000, \\\n",
    "                         tol = 1e-4, c = 1e-6, printPath = False) :\n",
    "    \"\"\"\n",
    "    This function computes the beta vector minimizing the objective function of the relaxed LASSO method,\n",
    "    using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : an Nxp-dimensionnal numpy array containing the predictors\n",
    "    target : an N-dimensionnal numpy array containing the target values\n",
    "    lbda : a float number - the penalty term\n",
    "    initialGuess : a p+1-dimensionnal numpy array - an initial guess for beta - defaulted to zero-vector\n",
    "    step : a float number - the stepsize of the gradient descent\n",
    "    maxIt : an integer - the maximum number of iteration used \n",
    "    tol : a float number - a tolerance for the gradient norm of the searched minimizing beta\n",
    "    c : a float number - a fixed parameter of Huber functions\n",
    "    printPath : a boolean value - if set to True the function prints the path taken by the optimization process\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : a p+1-dimensionnal numpy array - a beta minimizing the objective function\n",
    "    it : an integer - the number of iterations needed\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = predictors.shape # Retreiving shapes\n",
    "    \n",
    "    it = 0\n",
    "    \n",
    "    if initialGuess.shape[0] == 0 :\n",
    "        beta = np.zeros(p+1) # Default initial guess\n",
    "    \n",
    "    else :\n",
    "        beta = initialGuess\n",
    "        \n",
    "    for i in range(maxIt) :\n",
    "        \n",
    "        it += 1\n",
    "        \n",
    "        # Computing gradient\n",
    "        grad = gradLASSO(predictors = predictors, target = target, beta = beta, lbda = lbda, c = c)\n",
    "        gradNorm = np.linalg.norm(grad)\n",
    "        \n",
    "        if gradNorm < tol : # We arrived at a stationary point for given tolerance\n",
    "            break\n",
    "        \n",
    "        # Otherwise descent in the steepest direction\n",
    "        beta = beta - step * grad\n",
    "        \n",
    "        if printPath :\n",
    "            if i%(int(0.0025 * maxIt)) == 0 : # Prints from time to time \n",
    "                score = LASSOfunc(predictors = predictors, target = target, beta = beta, lbda = lbda, c = c)\n",
    "                print('Iteration ' + str(it) + ' | Function = ' + str(score) + \\\n",
    "                      ' | Gradient Norm = ' + str(gradNorm))\n",
    "        \n",
    "    return beta, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 | Function = 28442.378959477992 | Gradient Norm = 227.75490382111786\n",
      "Iteration 25001 | Function = 830.424658999481 | Gradient Norm = 0.036279485132313785\n",
      "Iteration 50001 | Function = 775.1844743713776 | Gradient Norm = 0.012610316646409759\n",
      "Iteration 75001 | Function = 768.0425521098945 | Gradient Norm = 0.004785049935000894\n",
      "Iteration 100001 | Function = 766.9711351830725 | Gradient Norm = 0.0019128563798617815\n",
      "Iteration 125001 | Function = 766.795754931298 | Gradient Norm = 0.0007878749380624969\n",
      "Iteration 150001 | Function = 766.7655810608005 | Gradient Norm = 0.0003302272927778618\n",
      "Iteration 175001 | Function = 766.7602358427371 | Gradient Norm = 0.0001399123353415887\n",
      "Iteration 200001 | Function = 766.7592716999577 | Gradient Norm = 5.9702761418182735e-05\n",
      "Iteration 225001 | Function = 766.7590958773776 | Gradient Norm = 2.5601027484080937e-05\n",
      "Iteration 250001 | Function = 766.7590636622168 | Gradient Norm = 1.1015526426888262e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.61614876e+00,  4.47752378e-02,  1.25879634e+00, -3.81497307e-02,\n",
       "         3.63076934e-01,  4.66848449e-03,  3.90507450e-01, -7.45834813e-02,\n",
       "        -3.57030878e-02, -1.52566764e-02, -1.83963849e-03]),\n",
       " 252874)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing gradient descent\n",
    "gradientDescentHuber(predictors = dataArray[:,:10], target = dataArray[:,10], \\\n",
    "                      lbda = 0.6, step = 1e-3, maxIt = 10000000, tol = 1e-5, \\\n",
    "                         c = 1e-6, printPath = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 | Function = 766.7590597190107 | Gradient Norm = 9.985721146600064e-06\n",
      "Iteration 25001 | Function = 766.7590549000959 | Gradient Norm = 4.231356969772427e-06\n",
      "Iteration 50001 | Function = 766.7590540150468 | Gradient Norm = 1.822314550042831e-06\n",
      "Iteration 75001 | Function = 766.7590538519114 | Gradient Norm = 7.867052342195796e-07\n",
      "Iteration 100001 | Function = 766.7590538220902 | Gradient Norm = 3.4013851444302373e-07\n",
      "Iteration 125001 | Function = 766.7590538167935 | Gradient Norm = 1.472084748134433e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.61637769e+00,  4.47142739e-02,  1.25871876e+00, -3.80099546e-02,\n",
       "         3.63073468e-01,  4.66533839e-03,  3.90510088e-01, -7.46027082e-02,\n",
       "        -3.57069269e-02, -1.52588058e-02, -1.80265209e-03]),\n",
       " 136551)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We pre-computed a 'good' beta minimizer to get quicker convergence \n",
    "# (This works even if there is a dependence on lambda)\n",
    "# We use this initial guess\n",
    "initialBeta = np.array([ 2.61614876e+00,  4.47752378e-02,  1.25879634e+00, -3.81497307e-02,\n",
    "         3.63076934e-01,  4.66848449e-03,  3.90507450e-01, -7.45834813e-02,\n",
    "        -3.57030878e-02, -1.52566764e-02, -1.83963849e-03])\n",
    "\n",
    "\n",
    "gradientDescentHuber(predictors = dataArray[:,:10], target = dataArray[:,10], initialGuess = initialBeta, \\\n",
    "                      lbda = 0.05, step = 1e-3, maxIt = 10000000, tol = 1e-7, \\\n",
    "                         c = 1e-6, printPath = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Finding optimal $\\lambda$\n",
    "\n",
    "We again use a 5-fold cross-validation process to find an optimal $\\lambda$ hyperparameter. We scan a range of possible lambdas computing the mean of mean squared errors, in order to find a suitable minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationLASSO(dataArray, lbda, initialGuess = initialBeta) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of mean squared errors of a five-fold cross-validation \n",
    "    of the relaxed LASSO regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : a float number - the penalty term\n",
    "    dataArray : an Nxp-dimensionnal numpy array containing both the predictors and the target value\n",
    "    initialGuess : a p+1-dimensionnal numpy array - an initial guess for beta - defaulted to our pre-computed\n",
    "        'best bet'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanMSE : a float number - the mean of mean squared errors of the five-fold cross-validation \n",
    "    \"\"\"\n",
    "    \n",
    "    meanSquaredErrors = np.zeros(5) # Container for the mean squared errors\n",
    "    \n",
    "    # Building the 5 subsets indexes\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        # Training\n",
    "        beta, _ = gradientDescentHuber(predictors = trainingSubset[:,:10], target = trainingSubset[:,10], \\\n",
    "                                    initialGuess = initialGuess, \\\n",
    "                      lbda = lbda, step = 1e-3, maxIt = 1000, tol = 1e-4, \\\n",
    "                         c = 1e-6, printPath = False)\n",
    "        # MSE on validation subset\n",
    "        meanSquaredErrors[i] = MSE(predictors = validationSubset[:,:10], target = validationSubset[:,10], \\\n",
    "                                   beta = beta)\n",
    "    \n",
    "    return np.mean(meanSquaredErrors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestLambdaLASSO(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter lambda by using a five-fold cross-validation\n",
    "    method for the relaxed LASSO regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nxp-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of mean squared errors and the tested lambdas \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_lambda : a float number - the optimal hyperparameter\n",
    "    lambdas : an M-dimensionnal numpy array containing the tested lambdas\n",
    "    lbda_MMSE : an M-dimensionnal numpy array containing the means of MSEs\n",
    "    best_lambda_ind : an integer - the optimal lambda index in lambdas\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    lambdas = np.linspace(start = 0.01, stop = 3400, num = 150) # Scanning lambdas\n",
    "    lbda_MMSE = np.zeros(lambdas.shape[0])\n",
    "    \n",
    "    for i, lbda in enumerate(lambdas) :\n",
    "        # Minimizing mean of MSE\n",
    "        lbda_MMSE[i] = fiveFoldValidationLASSO(dataArray = dataArray, lbda = lbda)\n",
    "        if i%25 == 0 : # Prints from time to time\n",
    "            print('Cross-validation for lambda = ' + str(lbda) + ' gives a mean of MSE = ' + str(lbda_MMSE[i]))\n",
    "        \n",
    "    best_lambda_ind = np.argmin(lbda_MMSE) # A minimizer of means of MSEs\n",
    "    \n",
    "    if returnArray :\n",
    "        return lambdas[best_lambda_ind], lambdas, lbda_MMSE, best_lambda_ind\n",
    "    \n",
    "    return lambdas[best_lambda_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for lambda = 0.01 gives a mean of MSE = 0.1875322390984032\n",
      "Cross-validation for lambda = 570.478120805369 gives a mean of MSE = 0.18753209357249018\n",
      "Cross-validation for lambda = 1140.946241610738 gives a mean of MSE = 0.1875311307787258\n",
      "Cross-validation for lambda = 1711.4143624161072 gives a mean of MSE = 0.1875307346404845\n",
      "Cross-validation for lambda = 2281.8824832214764 gives a mean of MSE = 0.1875302241046819\n",
      "Cross-validation for lambda = 2852.3506040268453 gives a mean of MSE = 0.18753053819813637\n"
     ]
    }
   ],
   "source": [
    "# Computing optimal lambda for LASSO\n",
    "bestLambdaLASSO, lambdas, lbda_MMSE, best_lambda_ind \\\n",
    " = chooseBestLambdaLASSO(dataArray = dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal λ = 2327.519932885906\n",
      "Minimum mean of MSEs = 0.18753018726076942\n"
     ]
    }
   ],
   "source": [
    "# Optimal lambda hyperparameter for LASSO\n",
    "print('Optimal λ = ' + str(bestLambdaLASSO))\n",
    "# Corresponding mean of MSEs\n",
    "print('Minimum mean of MSEs = ' + str(lbda_MMSE[best_lambda_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9cd666e50>]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting mean of MSEs against different lambdas : LASSO\n",
    "plt.figure()\n",
    "title = 'Mean of mean squared errors computed using five-fold cross-validation against hyperparameter $\\lambda$ \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Mean of MSEs')\n",
    "plt.plot(lambdas,lbda_MMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an optimal hyperparameter that is $\\lambda\\approx 2327.51$. We now compute the $R^2$ score for a $\\beta$ vector obtained on the full data set. We also compute the score on the unused testing data set, using the optimal $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_LASSO =  [ 2.59244275e+00  3.70541093e-02  1.25512614e+00 -2.51006000e-03\n",
      "  3.61845960e-01  1.81569736e-03  3.94104704e-01 -7.61873140e-02\n",
      " -3.65516622e-02 -1.54841140e-02  7.22181153e-07]\n"
     ]
    }
   ],
   "source": [
    "# Training on full dataset - LASSO Regression\n",
    "betaLASSO, _ = gradientDescentHuber(predictors = dataArray[:,:10], target = dataArray[:,10], lbda = bestLambdaLASSO, \\\n",
    "                     initialGuess = initialBeta, step = 1e-3, maxIt = 100000, \\\n",
    "                         tol = 1e-7, c = 1e-6, printPath = False)\n",
    "print('beta_LASSO = ',betaLASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 with LASSO on full data set =  0.8717725639585844\n",
      "Previous (Linear Regression) R^2 = 0.8718259975718018\n",
      "Difference = -5.343361321741291e-05\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 with best lambda on full data set\n",
    "RsquaredTrainingLASSO = getRsquared(predictors = dataArray[:,:10], target = dataArray[:,10], \\\n",
    "                                    beta = betaLASSO)\n",
    "print('R^2 with LASSO on full data set = ',RsquaredTrainingLASSO)\n",
    "print('Previous (Linear Regression) R^2 =', RsquaredTraining)\n",
    "print('Difference =', RsquaredTrainingLASSO - RsquaredTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 with LASSO on testing set =  0.8641651128285768\n",
      "Previous (Linear Regression) R^2 = 0.8642933369927278\n",
      "Difference = -0.0001282241641510362\n"
     ]
    }
   ],
   "source": [
    "# Computing R^2 with best lambda on testing data set\n",
    "RsquaredTestingLASSO = getRsquared(predictors = testingDataArray[:,:10], target = testingDataArray[:,10],\\\n",
    "                                   beta = betaLASSO)\n",
    "print('R^2 with LASSO on testing set = ',RsquaredTestingLASSO)\n",
    "print('Previous (Linear Regression) R^2 =', RsquaredTesting)\n",
    "print('Difference =', RsquaredTestingLASSO - RsquaredTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Comparing regression coefficients\n",
    "\n",
    "We analyze the different $\\beta$ obtained with the relaxed LASSO method and the Ridge regression method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of LASSO :  [ 2.59244275e+00  3.70541093e-02  1.25512614e+00 -2.51006000e-03\n",
      "  3.61845960e-01  1.81569736e-03  3.94104704e-01 -7.61873140e-02\n",
      " -3.65516622e-02 -1.54841140e-02  7.22181153e-07]\n",
      "\n",
      "Coefficients of RIDGE :  [ 2.61821445  0.04433401  1.25230457 -0.03780147  0.36279609  0.00495367\n",
      "  0.39065473 -0.07463672 -0.03570967 -0.01524778  0.00319929]\n",
      "\n",
      "Norm of beta RIDGE - Norm of beta LASSO =  0.021574597198687595\n"
     ]
    }
   ],
   "source": [
    "# Comparing beta of LASSO and RIDGE regression\n",
    "print('Coefficients of LASSO : ',betaLASSO)\n",
    "print('')\n",
    "print('Coefficients of RIDGE : ',betaRIDGE)\n",
    "print('')\n",
    "print('Norm of beta RIDGE - Norm of beta LASSO = ', np.linalg.norm(betaRIDGE) - np.linalg.norm(betaLASSO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said previously, both method used here are shrinkage methods that incite coefficients to be small. As Ridge regression will constrain some coefficients to be small-scale but not zero, the Lasso method will enforce many coefficients to be zero, leaving some other non-zero entries (for instance see the difference between $\\beta^{RIDGE}_{11}\\approx 0.003$ and $\\beta^{LASSO}_{11}\\approx 10^{-7}$).\n",
    "\n",
    "Both methods reduce accuracy on the training set. However, Ridge regression will lower overfitting and provide a better generalizability of the model, while the Lasso method will better select a minimum set of non-covariate predictors that correlate best with the target, thus informing the user on the underlying phenomenon at stake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi0I8ERdy0mD"
   },
   "source": [
    "<a name=\"task-2\"></a>\n",
    "\n",
    "# Task 2: Classification [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgBfTmgpy08Z"
   },
   "source": [
    "<a name=\"q21\"></a>\n",
    "\n",
    "## 2.1 kNN classifier [^](#outline)\n",
    "\n",
    "Our goal here is to predict a 2-classes categorical output based on a 30-features input. This classification task is made to predict the diagnosis of breast tumour samples as ‘benign’ ('B') or ‘malignant’ ('M'). We use features and targets available in the $\\texttt{tumour_samples.csv}$ dataset file. \n",
    "\n",
    "We start by visualizing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.855170</td>\n",
       "      <td>15.248290</td>\n",
       "      <td>69.167041</td>\n",
       "      <td>359.534878</td>\n",
       "      <td>0.105488</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>...</td>\n",
       "      <td>18.984557</td>\n",
       "      <td>81.443134</td>\n",
       "      <td>466.879302</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.335999</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.068958</td>\n",
       "      <td>15.532758</td>\n",
       "      <td>66.130635</td>\n",
       "      <td>330.040665</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>...</td>\n",
       "      <td>22.840293</td>\n",
       "      <td>82.133171</td>\n",
       "      <td>473.367822</td>\n",
       "      <td>0.125478</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.312882</td>\n",
       "      <td>0.096158</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.271409</td>\n",
       "      <td>18.100314</td>\n",
       "      <td>78.195610</td>\n",
       "      <td>421.537832</td>\n",
       "      <td>0.105147</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.043317</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.188801</td>\n",
       "      <td>...</td>\n",
       "      <td>26.365608</td>\n",
       "      <td>84.598334</td>\n",
       "      <td>620.586067</td>\n",
       "      <td>0.146766</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.291805</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.531733</td>\n",
       "      <td>18.452486</td>\n",
       "      <td>67.227069</td>\n",
       "      <td>340.063033</td>\n",
       "      <td>0.086041</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.203093</td>\n",
       "      <td>...</td>\n",
       "      <td>24.385385</td>\n",
       "      <td>73.296855</td>\n",
       "      <td>429.675600</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>0.177225</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.287749</td>\n",
       "      <td>0.073174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.367686</td>\n",
       "      <td>14.399191</td>\n",
       "      <td>80.643814</td>\n",
       "      <td>460.849710</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.195326</td>\n",
       "      <td>...</td>\n",
       "      <td>19.614305</td>\n",
       "      <td>89.910502</td>\n",
       "      <td>472.323112</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  n1_radius  n1_texture  n1_perimeter     n1_area  n1_smoothness  \\\n",
       "0           0  10.855170   15.248290     69.167041  359.534878       0.105488   \n",
       "1           1  10.068958   15.532758     66.130635  330.040665       0.099813   \n",
       "2           2  12.271409   18.100314     78.195610  421.537832       0.105147   \n",
       "3           3  10.531733   18.452486     67.227069  340.063033       0.086041   \n",
       "4           4  12.367686   14.399191     80.643814  460.849710       0.106410   \n",
       "\n",
       "   n1_compactness  n1_concavity  n1_concave_points  n1_symmetry  ...  \\\n",
       "0        0.080200      0.066410           0.034194     0.182796  ...   \n",
       "1        0.109540      0.057583           0.023322     0.195650  ...   \n",
       "2        0.095315      0.043317           0.031539     0.188801  ...   \n",
       "3        0.049961      0.049709           0.011046     0.203093  ...   \n",
       "4        0.101420      0.020806           0.021990     0.195326  ...   \n",
       "\n",
       "   n3_texture  n3_perimeter     n3_area  n3_smoothness  n3_compactness  \\\n",
       "0   18.984557     81.443134  466.879302       0.149080        0.200185   \n",
       "1   22.840293     82.133171  473.367822       0.125478        0.330466   \n",
       "2   26.365608     84.598334  620.586067       0.146766        0.118707   \n",
       "3   24.385385     73.296855  429.675600       0.100060        0.143683   \n",
       "4   19.614305     89.910502  472.323112       0.138135        0.276127   \n",
       "\n",
       "   n3_concavity  n3_concave_points  n3_symmetry  n3_fractal_dimension  \\\n",
       "0      0.205695           0.111592     0.335999              0.093477   \n",
       "1      0.283304           0.088021     0.312882              0.096158   \n",
       "2      0.147900           0.050402     0.291805              0.069556   \n",
       "3      0.177225           0.028111     0.287749              0.073174   \n",
       "4      0.151098           0.074396     0.345258              0.095830   \n",
       "\n",
       "   DIAGNOSIS  \n",
       "0          B  \n",
       "1          B  \n",
       "2          B  \n",
       "3          B  \n",
       "4          B  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tumour data into a pandas dataframe \n",
    "df_tumour = pd.read_csv(\"tumour_samples.csv\")\n",
    "# Displaying the first five rows\n",
    "df_tumour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 2566\n"
     ]
    }
   ],
   "source": [
    "# Finding number of samples\n",
    "nb_samples = df_tumour['n1_radius'].count()\n",
    "print('Number of samples : ' + str(nb_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Training the k-NN\n",
    "\n",
    "For a new entry feature variable $\\mathbf{x}_i\\in\\mathbb{R}^{30}$, this method computes its $k$-nearest-neigbors in the training set - here in the sense of the euclidian norm - and further uses the majority rule upon the classes of said neigbors to provide a predicted class for this unseen inputed variable. \n",
    "\n",
    "We will here also consider that the training set has been cleaned and pre-processed accordingly.\n",
    "\n",
    "In the following, we build a set of functions to implement this method and we use a 5-fold cross-validation process to find the optimal hyperparameter $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our dataframe to a numpy array\n",
    "dataArray = df_tumour.to_numpy()\n",
    "# Standardization\n",
    "dataArray[:,1:31] = (dataArray[:,1:31] - np.mean(dataArray[:,1:31])) / np.std(dataArray[:,1:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbors(inputFeatures, k, predictors = dataArray) :\n",
    "    \"\"\"\n",
    "    This function computes the indexes of the k-nearest-neighbors of the inputFeatures vector in the\n",
    "    predictors array, with respect to the euclidean distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputFeatures : a p-dimensionnal numpy array - the input vector for which we compute the neighbors\n",
    "    k : an integer - the number of neighbors searched for\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value - an \n",
    "        identifier index is also set at the begining of the rows \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    neighborsIndexes : a k-dimensionnal numpy array - the indexes of the k-closest neighbors\n",
    "    \"\"\"\n",
    "    distances = np.zeros(len(predictors)) # Initializing\n",
    "    \n",
    "    for ind, predictor in enumerate(predictors) :\n",
    "        distances[ind] = 1 / np.linalg.norm(predictor[1:31] - inputFeatures) # Computing all distances \n",
    "        # We will look for the argmax - we thus take 1/distance\n",
    "    \n",
    "    neighborsIndexes = np.argpartition(distances, -k)[-k:] # Indexes of k lowest distances\n",
    "        \n",
    "    return neighborsIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(inputFeatures, k, predictors = dataArray) :\n",
    "    \"\"\"\n",
    "    This function computes a prediction for the inputFeatures vector, based on the majority rule \n",
    "    applied to its k-nearest-neighbors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputFeatures : a p-dimensionnal numpy array - the input vector for which we compute the prediction\n",
    "    k : an integer - the number of neighbors\n",
    "    predictors : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value - an \n",
    "        identifier index is also set at the begining of the rows \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : a string or an integer - the prediction - -1 is returned if the majority rule cannot be applied\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computing nearest neighbors\n",
    "    neighborsIndexes = getNeighbors(inputFeatures, k, predictors) \n",
    "    neighbors = predictors[neighborsIndexes]\n",
    "    \n",
    "    # Counting the number of 'M' and 'B' classes in the neighbors \n",
    "    nbM = list(neighbors[:,31]).count('M')\n",
    "    nbB = list(neighbors[:,31]).count('B')\n",
    "    \n",
    "    # Majority rule\n",
    "    if nbM > nbB :\n",
    "        return 'M'\n",
    "    elif nbB > nbM :\n",
    "        return 'B'\n",
    "    else : # If the number of 'M' equals the number of 'B' we abandon the prediction \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the optimal $k$ by using the cross-validation method, we here measure accuracy, that is to say the proportion of correct predictions made over the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationKNN(dataArray, k) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of accuracies of a five-fold cross-validation process made\n",
    "    on the KNN model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    k : an integer - the number of neighbors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanAccuracy : a float number - the mean of accuracies of the five-fold cross-validation \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = np.zeros(5) # Container for the accuracies\n",
    "\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        \n",
    "        accuracy = 0\n",
    "        # Training and Validation\n",
    "        for inputFeatures in validationSubset :\n",
    "            prediction = kNN(inputFeatures = inputFeatures[1:31], k = k, predictors = trainingSubset)\n",
    "            if prediction != -1 : # Ties are considered as a missed prediction here \n",
    "                if prediction == inputFeatures[31] :\n",
    "                    accuracy += 1\n",
    "        accuracies[i] = accuracy / len(validationSubset)\n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestK(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter k by using a five-fold cross-validation\n",
    "    method for the kNN model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of accuracies and the tested k \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_k : a integer - the optimal hyperparameter\n",
    "    kList : an M-dimensionnal numpy array containing the tested k\n",
    "    meanAccuracies : an M-dimensionnal numpy array containing the means of accuracies\n",
    "    best_k_ind : an integer - the optimal k index in kList\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    kList = np.arange(start = 1, stop = 8, step = 1) # Scanning k\n",
    "    meanAccuracies = np.zeros(kList.shape[0])\n",
    "    \n",
    "    for i, k in enumerate(kList) :\n",
    "        # Maximizing mean of accuracies\n",
    "        meanAccuracies[i] = fiveFoldValidationKNN(dataArray = dataArray, k = k)\n",
    "        print('Cross-validation for k = ' + str(k) + ' gives a mean of accuracies = ' + str(meanAccuracies[i]))\n",
    "        \n",
    "    best_k_ind = np.argmax(meanAccuracies) # A maximizer of accuracies \n",
    "    \n",
    "    if returnArray :\n",
    "        return kList[best_k_ind], kList, meanAccuracies, best_k_ind\n",
    "    \n",
    "    return kList[best_k_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for k = 1 gives a mean of accuracies = 0.9586744639376217\n",
      "Cross-validation for k = 2 gives a mean of accuracies = 0.9391812865497075\n",
      "Cross-validation for k = 3 gives a mean of accuracies = 0.9629629629629628\n",
      "Cross-validation for k = 4 gives a mean of accuracies = 0.9536062378167642\n",
      "Cross-validation for k = 5 gives a mean of accuracies = 0.9614035087719298\n",
      "Cross-validation for k = 6 gives a mean of accuracies = 0.9551656920077972\n",
      "Cross-validation for k = 7 gives a mean of accuracies = 0.9614035087719298\n"
     ]
    }
   ],
   "source": [
    "# Computing optimal k for KNN\n",
    "bestK, kList, meanAccuracies, best_k_ind \\\n",
    " = chooseBestK(dataArray = dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k = 3\n",
      "Maximum mean of accuracies = 0.9629629629629628\n"
     ]
    }
   ],
   "source": [
    "# Optimal k hyperparameter for kNN\n",
    "print('Optimal k = ' + str(bestK))\n",
    "# Corresponding mean of MSEs\n",
    "print('Maximum mean of accuracies = ' + str(meanAccuracies[best_k_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Computing accuracies\n",
    "\n",
    "We compute the accuracies of our model having fixed $k$ to an optimum ($k=3$), on both the training and the unseen testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyKNN(testingArray, k, trainingArray = dataArray) :\n",
    "    \"\"\"\n",
    "    This function computes the accuracy of the KNN model, trained on the dataArray.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    testingArray : an Nx(p+2)-dimensionnal numpy array contain containing both the predictors and \n",
    "        the target value of the testing set \n",
    "    k : an integer - the number of neighbors\n",
    "    trainingArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and \n",
    "        the target value of the training set \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : a float number - the accuracy of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    # Training and Testing\n",
    "    for inputFeatures in testingArray :\n",
    "        prediction = kNN(inputFeatures = inputFeatures[1:31], k = k, predictors = trainingArray)\n",
    "        if prediction != -1 : # Ties are considered as a missed prediction here \n",
    "            if prediction == inputFeatures[31] :\n",
    "                accuracy += 1\n",
    "    accuracy = accuracy / len(testingArray)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full training data = 0.975448168355417\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy on full training data \n",
    "accuracyTraining = accuracyKNN(testingArray = dataArray, k = bestK)\n",
    "print('Accuracy on full training data = ' + str(accuracyTraining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>20.52</td>\n",
       "      <td>88.64</td>\n",
       "      <td>556.7</td>\n",
       "      <td>0.11060</td>\n",
       "      <td>0.14690</td>\n",
       "      <td>0.14450</td>\n",
       "      <td>0.08172</td>\n",
       "      <td>0.2116</td>\n",
       "      <td>...</td>\n",
       "      <td>29.66</td>\n",
       "      <td>113.30</td>\n",
       "      <td>844.4</td>\n",
       "      <td>0.15740</td>\n",
       "      <td>0.38560</td>\n",
       "      <td>0.51060</td>\n",
       "      <td>0.20510</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.21</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.02772</td>\n",
       "      <td>0.02068</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>...</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.06005</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.02</td>\n",
       "      <td>15.66</td>\n",
       "      <td>89.59</td>\n",
       "      <td>606.5</td>\n",
       "      <td>0.07966</td>\n",
       "      <td>0.05581</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.02652</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>...</td>\n",
       "      <td>19.31</td>\n",
       "      <td>96.53</td>\n",
       "      <td>688.9</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.06260</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>0.06710</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14.26</td>\n",
       "      <td>18.17</td>\n",
       "      <td>91.22</td>\n",
       "      <td>633.1</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>0.05220</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>0.01374</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>...</td>\n",
       "      <td>25.26</td>\n",
       "      <td>105.80</td>\n",
       "      <td>819.7</td>\n",
       "      <td>0.09445</td>\n",
       "      <td>0.21670</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.07530</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>0.07676</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13.03</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.02923</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  n1_radius  n1_texture  n1_perimeter  n1_area  n1_smoothness  \\\n",
       "0           0      13.40       20.52         88.64    556.7        0.11060   \n",
       "1           1      13.21       25.25         84.10    537.9        0.08791   \n",
       "2           2      14.02       15.66         89.59    606.5        0.07966   \n",
       "3           3      14.26       18.17         91.22    633.1        0.06576   \n",
       "4           4      13.03       18.42         82.61    523.8        0.08983   \n",
       "\n",
       "   n1_compactness  n1_concavity  n1_concave_points  n1_symmetry  ...  \\\n",
       "0         0.14690       0.14450            0.08172       0.2116  ...   \n",
       "1         0.05205       0.02772            0.02068       0.1619  ...   \n",
       "2         0.05581       0.02087            0.02652       0.1589  ...   \n",
       "3         0.05220       0.02475            0.01374       0.1635  ...   \n",
       "4         0.03766       0.02562            0.02923       0.1467  ...   \n",
       "\n",
       "   n3_texture  n3_perimeter  n3_area  n3_smoothness  n3_compactness  \\\n",
       "0       29.66        113.30    844.4        0.15740         0.38560   \n",
       "1       34.23         91.29    632.9        0.12890         0.10630   \n",
       "2       19.31         96.53    688.9        0.10340         0.10170   \n",
       "3       25.26        105.80    819.7        0.09445         0.21670   \n",
       "4       22.81         84.46    545.9        0.09701         0.04619   \n",
       "\n",
       "   n3_concavity  n3_concave_points  n3_symmetry  n3_fractal_dimension  \\\n",
       "0       0.51060            0.20510       0.3585               0.11090   \n",
       "1       0.13900            0.06005       0.2444               0.06788   \n",
       "2       0.06260            0.08216       0.2136               0.06710   \n",
       "3       0.15650            0.07530       0.2636               0.07676   \n",
       "4       0.04833            0.05013       0.1987               0.06169   \n",
       "\n",
       "   DIAGNOSIS  \n",
       "0          M  \n",
       "1          B  \n",
       "2          B  \n",
       "3          B  \n",
       "4          B  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing tumour data into a pandas dataframe \n",
    "df_tumour_test = pd.read_csv(\"tumour_test.csv\")\n",
    "# Displaying the first five rows\n",
    "df_tumour_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 200\n"
     ]
    }
   ],
   "source": [
    "# Finding number of samples\n",
    "nb_samples = df_tumour_test['n1_radius'].count()\n",
    "print('Number of samples : ' + str(nb_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our dataframe to a numpy array\n",
    "testingDataArray = df_tumour_test.to_numpy()\n",
    "# Standardization\n",
    "testingDataArray[:,1:31] = (testingDataArray[:,1:31] - \\\n",
    "                            np.mean(testingDataArray[:,1:31])) / np.std(testingDataArray[:,1:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen testing data = 0.875\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy on unseen testing data \n",
    "accuracyTesting = accuracyKNN(testingArray = testingDataArray, k = bestK)\n",
    "print('Accuracy on unseen testing data = ' + str(accuracyTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the accuracy computed on the testing set is rather smaller than the one computed on the training set. Unsurprisingly, the model's attempts to predict unseen data are less likely to be valid, however the accuracy remains high (87%), specially considering the small amount of data available in the testing file (200 samples), on which said score has been computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCPlajQOy1ER"
   },
   "source": [
    "<a name=\"q22\"></a>\n",
    "\n",
    "## 2.2 Random forest [^](#outline)\n",
    "\n",
    "We use a random forest method to perform a classification task on the same data set of tumour samples. Namely, we build decision trees that succesively split the input variable space by minimizing the $\\textit{cross-entropy}$ function, further partionning said space. We then use a majority rule to obtain a probability vector describing the chances a given input has from belonging to a specific class.\n",
    "\n",
    "We randomly sample our training set and generate $B$ decision trees, which outcomes are aggregated to produce our prediction. Moreover, the splits building our decision trees are made minimizing cross-entropy over a random sample of possible predictors to be chosen for the operation (feature bagging).\n",
    "\n",
    "In the following, we build several helper functions to perform this task.\n",
    "\n",
    "### 2.2.1 Training \n",
    "\n",
    "We use the same 5-fold cross-validation process to fix the number of trees and tree-depth hyperparameters. We use accuracy to measure the performance of the model.\n",
    "\n",
    "As the implementation becomes more complex, we now heavily $\\textbf{rely on the weekly coding tasks solutions}$, and further $\\textbf{modify them to suit our current problem}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the weekly Coding Task solution functions, you may find my custom comments in the form of ### _____ ###\n",
    "\n",
    "# CT solution introduce the possibility of setting non uniform sample weights\n",
    "# We won't consider this case here\n",
    "uniform_weights = np.ones(dataArray.shape[0]) / dataArray.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING CT6 SOLUTIONS ## \n",
    "\n",
    "### Gini-index modified to cross-entropy ###\n",
    "def cross_entropy(y, sample_weights = uniform_weights):\n",
    "  \"\"\" \n",
    "  Calculate the cross-entropy for labels.\n",
    "  Arguments:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (float): the cross-entropy for y.\n",
    "  \"\"\"\n",
    "\n",
    "  ### Weights are uniform here ### \n",
    "\n",
    "  # count different labels in y，and store in label_weights\n",
    "  # initialize with zero for each distinct label.\n",
    "  label_weights = {yi: 0 for yi in set(y)}  \n",
    "  for yi, wi in zip(y, sample_weights):\n",
    "      label_weights[yi] += wi\n",
    "\n",
    "  total_weight = sum(label_weights.values())\n",
    "\n",
    "  ### We now use the cross-entropy formula ### \n",
    "  CE = 0 \n",
    "  for label, weight in label_weights.items():\n",
    "      probability = (weight / total_weight)\n",
    "      CE += probability * np.log(probability)\n",
    "\n",
    "  return -CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE =  0.4482540468603776\n"
     ]
    }
   ],
   "source": [
    "# Trying cross-entropy over the entire data set\n",
    "print('CE = ', cross_entropy(y = dataArray[:,31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT6 SOLUTION FUNCTION ## \n",
    "\n",
    "def split_samples(X, y, sample_weights, column, value, categorical):\n",
    "  \"\"\"\n",
    "  Return the split of data whose column-th feature:\n",
    "    1. equals value, in case `column` is categorical, or\n",
    "    2. less than value, in case `column` is not categorical (i.e. numerical)\n",
    "\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for splitting.\n",
    "      value: splitting threshold  the samples \n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Returns:\n",
    "      tuple(np.array, np.array): tuple of subsets of X splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of y splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of sample weights based on column-th value.\n",
    "  \"\"\" \n",
    "\n",
    "  if categorical:\n",
    "    left_mask =(X[:, column] == value)\n",
    "  else:\n",
    "    left_mask = (X[:, column] < value)\n",
    "  \n",
    "  X_left, X_right = X[left_mask, :], X[~left_mask, :]\n",
    "  y_left, y_right = y[left_mask], y[~left_mask]\n",
    "  w_left, w_right  = sample_weights[left_mask], sample_weights[~left_mask]\n",
    "\n",
    "  return (X_left, X_right), (y_left, y_right), (w_left, w_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING CT6 SOLUTIONS ## \n",
    "### Now using cross-entropy ###\n",
    "\n",
    "def CE_split_value(X, y, sample_weights, column, categorical):\n",
    "  \"\"\"\n",
    "  Calculate the cross-entropy based on `column` with the split that minimizes the cross-entropy.\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for calculating. 0 <= column < D\n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Returns:\n",
    "      (float, float): the resulted cross-entropy and the corresponding value used in splitting.\n",
    "  \"\"\"\n",
    "  \n",
    "  unique_vals = np.unique(X[:, column])\n",
    "\n",
    "  assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
    "\n",
    "  CE_val, threshold = np.inf, None\n",
    "  \n",
    "  # split the values of i-th feature and calculate the cost \n",
    "  for value in unique_vals:\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, column, value, categorical) \n",
    "\n",
    "    # if one of the two sides is empty, skip this split.\n",
    "    if len(y_l) == 0 or len(y_r) == 0:\n",
    "      continue\n",
    "    \n",
    "    ### Now using cross-entropy ###\n",
    "    p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
    "    p_right = 1 - p_left\n",
    "    new_cost = p_left * cross_entropy(y_l, w_l) + p_right * cross_entropy(y_r, w_r)\n",
    "    \n",
    "    if new_cost < CE_val:\n",
    "      CE_val, threshold = new_cost, value\n",
    "    \n",
    "  return CE_val, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3461197233140173, -0.27976282172399447)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying out a split on x_5\n",
    "CE_split_value(X = dataArray[:,1:31], y = dataArray[:,31], sample_weights = uniform_weights, \\\n",
    "               column = 5, categorical = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT6 SOLUTION FUNCTION ## \n",
    "\n",
    "def CE_split(X, y, sample_weights, columns_dict):\n",
    "  \"\"\"\n",
    "  Choose the best feature to split according to criterion.\n",
    "  Args:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "  Returns:\n",
    "      (int, float): the best feature index and value used in splitting. \n",
    "      If the feature index is None, then no valid split for the current Node.\n",
    "  \"\"\"\n",
    "\n",
    "  \n",
    "  min_CE, split_column, split_val = np.inf, None, 0\n",
    "  for column, categorical in columns_dict.items():\n",
    "      # skip column if samples are not seperable by that column.\n",
    "      if len(np.unique(X[:, column])) < 2:\n",
    "        continue\n",
    "      CE, val = CE_split_value(X, y, sample_weights, column, categorical)      \n",
    "      if CE < min_CE:\n",
    "          min_CE, split_column, split_val = CE, column, val\n",
    "\n",
    "  return split_column, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT6 SOLUTION FUNCTION ## \n",
    "\n",
    "def CE_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
    "  \"\"\"\n",
    "  Choose the best feature to split according to criterion.\n",
    "  Args:\n",
    "      n_features: number of sampled features.\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "  Returns:\n",
    "      (float, int, float): the minimized cross-entropy, the best feature index and value used in splitting.\n",
    "  \"\"\"\n",
    "  columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
    "  columns_dict = {c: columns_dict[c] for c in columns}\n",
    "\n",
    "  min_CE, split_column, split_val = np.inf, 0, 0\n",
    "  for column, categorical in columns_dict.items():\n",
    "    # skip column if samples are not seperable by that column.\n",
    "    if len(np.unique(X[:, column])) < 2:\n",
    "      continue\n",
    "\n",
    "    # search for the best splitting value for the given column.\n",
    "    CE, val = CE_split_value(X, y, sample_weights, column, categorical)      \n",
    "    if CE < min_CE:\n",
    "        min_CE, split_column, split_val = CE, column, val\n",
    "\n",
    "  return min_CE, split_column, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor index to perform the split on =  22\n",
      "Threshold =  0.3614633428241891\n"
     ]
    }
   ],
   "source": [
    "# All predictors are continuous in our case \n",
    "columns_dict = {index: False for index in range(dataArray[:,1:31].shape[1])}\n",
    "\n",
    "# Trying the split - Which descriptor is best to perform the split on, what is the threshold ?\n",
    "split_column, split_val = CE_split(X = dataArray[:,1:31], \\\n",
    "                                   y = dataArray[:,31], sample_weights = uniform_weights,\\\n",
    "                                   columns_dict = columns_dict)\n",
    "\n",
    "print('Predictor index to perform the split on = ',split_column)\n",
    "print('Threshold = ',split_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor index to perform the split on =  3\n",
      "Threshold =  3.5793188329304426\n"
     ]
    }
   ],
   "source": [
    "# Which descriptor is best if we try splits on a random set of descriptors ?\n",
    "# What is the corresonding threshold ?\n",
    "_ , split_column, split_val = CE_split_rf(n_features = 10, X = dataArray[:,1:31], \\\n",
    "                                   y = dataArray[:,31], sample_weights = uniform_weights,\\\n",
    "                                   columns_dict = columns_dict)\n",
    "print('Predictor index to perform the split on = ',split_column)\n",
    "print('Threshold = ',split_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT6 SOLUTION FUNCTION ## \n",
    "\n",
    "def majority_vote(y, sample_weights):\n",
    "  \"\"\"\n",
    "  Return the label which appears the most in y.\n",
    "  Args:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (int): the majority label\n",
    "  \"\"\"\n",
    "  majority_label = {yi: 0 for yi in set(y)}\n",
    "\n",
    "  for yi, wi in zip(y, sample_weights):\n",
    "    majority_label[yi] += wi\n",
    "  return max(majority_label, key=majority_label.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIGHTLY MODIFYING THIS CT6 SOLUTION FUNCTION ## \n",
    "### Dropping feature name ###\n",
    "\n",
    "\n",
    "def build_tree_rf(n_features, X, y, sample_weights, \\\n",
    "                  columns_dict, depth,  max_depth=10, min_samples_leaf=2):\n",
    "  \"\"\"Build the decision tree according to the data.\n",
    "  Args:\n",
    "      X: (np.array) training features, of shape (N, D).\n",
    "      y: (np.array) vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      feature_names (list): record the name of features in X in the original dataset.\n",
    "      depth (int): current depth for this node.\n",
    "  Returns:\n",
    "      (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "        1. 'feature_name': The column name of the split.\n",
    "        2. 'feature_index': The column index of the split.\n",
    "        3. 'value': The value used for the split.\n",
    "        4. 'categorical': indicator for categorical/numerical variables.\n",
    "        5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "        6. 'left': The left sub-tree with the same structure.\n",
    "        7. 'right' The right sub-tree with the same structure.\n",
    "      Example:\n",
    "          mytree = {\n",
    "              'feature_index': 2,\n",
    "              'value': 3.0,\n",
    "              'categorical': False,\n",
    "              'majority_label': None,\n",
    "              'left': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "              'right': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "          }\n",
    "  \"\"\"\n",
    "  # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
    "  if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "  \n",
    "  else:\n",
    "    CE, split_index, split_val = CE_split_rf(n_features, X, y, sample_weights, columns_dict)\n",
    "    \n",
    "    # If CE is infinity, it means that samples are not seperable by the sampled features.\n",
    "    if CE == np.inf:\n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "    categorical = columns_dict[split_index]\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, split_index, split_val, categorical)\n",
    "    return {\n",
    "        'feature_index': split_index, ### Dropping feature name ###\n",
    "        'value': split_val,\n",
    "        'categorical': categorical,\n",
    "        'majority_label': None,\n",
    "        'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, depth + 1, max_depth, min_samples_leaf),\n",
    "        'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, depth + 1, max_depth, min_samples_leaf)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_index': 27,\n",
       " 'value': -0.279510198052512,\n",
       " 'categorical': False,\n",
       " 'majority_label': None,\n",
       " 'left': {'feature_index': 13,\n",
       "  'value': -0.09145766210249551,\n",
       "  'categorical': False,\n",
       "  'majority_label': None,\n",
       "  'left': {'feature_index': 0,\n",
       "   'value': -0.19479106778623373,\n",
       "   'categorical': False,\n",
       "   'majority_label': None,\n",
       "   'left': {'feature_index': 22,\n",
       "    'value': 0.29935126648984833,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'feature_index': 18,\n",
       "     'value': -0.28024820848257287,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 7,\n",
       "      'value': -0.28002689373704853,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'feature_index': 23,\n",
       "       'value': 3.952424768602475,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'majority_label': 'B'},\n",
       "       'right': {'feature_index': 3,\n",
       "        'value': 2.6409421582238193,\n",
       "        'categorical': False,\n",
       "        'majority_label': None,\n",
       "        'left': {'majority_label': 'M'},\n",
       "        'right': {'majority_label': 'B'}}},\n",
       "      'right': {'feature_index': 11,\n",
       "       'value': -0.2725854247192171,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'majority_label': 'B'},\n",
       "       'right': {'majority_label': 'M'}}},\n",
       "     'right': {'majority_label': 'B'}},\n",
       "    'right': {'feature_index': 13,\n",
       "     'value': -0.2017146562736729,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 0,\n",
       "      'value': -0.19669715033645002,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'M'},\n",
       "      'right': {'majority_label': 'B'}},\n",
       "     'right': {'feature_index': 24,\n",
       "      'value': -0.27953098356990114,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'feature_index': 23,\n",
       "       'value': 3.884391185654139,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'feature_index': 18,\n",
       "        'value': -0.2802433942210704,\n",
       "        'categorical': False,\n",
       "        'majority_label': None,\n",
       "        'left': {'feature_index': 10,\n",
       "         'value': -0.2787335082705997,\n",
       "         'categorical': False,\n",
       "         'majority_label': None,\n",
       "         'left': {'majority_label': 'B'},\n",
       "         'right': {'majority_label': 'M'}},\n",
       "        'right': {'majority_label': 'B'}},\n",
       "       'right': {'majority_label': 'B'}},\n",
       "      'right': {'feature_index': 11,\n",
       "       'value': -0.27643936528492047,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'majority_label': 'B'},\n",
       "       'right': {'majority_label': 'M'}}}}},\n",
       "   'right': {'feature_index': 22,\n",
       "    'value': 0.38844595008236826,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'feature_index': 14,\n",
       "     'value': -0.28030486338962013,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'majority_label': 'M'},\n",
       "     'right': {'feature_index': 29,\n",
       "      'value': -0.279979167334013,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'M'},\n",
       "      'right': {'majority_label': 'B'}}},\n",
       "    'right': {'majority_label': 'M'}}},\n",
       "  'right': {'feature_index': 0,\n",
       "   'value': -0.1973277467004694,\n",
       "   'categorical': False,\n",
       "   'majority_label': None,\n",
       "   'left': {'feature_index': 22,\n",
       "    'value': 0.22941163415997476,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'feature_index': 15,\n",
       "     'value': -0.28027569142599895,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 18,\n",
       "      'value': -0.2801958708273001,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'M'},\n",
       "      'right': {'majority_label': 'B'}},\n",
       "     'right': {'majority_label': 'B'}},\n",
       "    'right': {'feature_index': 23,\n",
       "     'value': 4.544754285264567,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 1,\n",
       "      'value': -0.18386868078655869,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'B'},\n",
       "      'right': {'feature_index': 27,\n",
       "       'value': -0.27968600935746823,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'feature_index': 23,\n",
       "        'value': 3.1879050912598093,\n",
       "        'categorical': False,\n",
       "        'majority_label': None,\n",
       "        'left': {'majority_label': 'M'},\n",
       "        'right': {'feature_index': 27,\n",
       "         'value': -0.27971937410505704,\n",
       "         'categorical': False,\n",
       "         'majority_label': None,\n",
       "         'left': {'majority_label': 'B'},\n",
       "         'right': {'majority_label': 'B'}}},\n",
       "       'right': {'feature_index': 18,\n",
       "        'value': -0.2800983138116844,\n",
       "        'categorical': False,\n",
       "        'majority_label': None,\n",
       "        'left': {'majority_label': 'M'},\n",
       "        'right': {'majority_label': 'B'}}}},\n",
       "     'right': {'majority_label': 'M'}}},\n",
       "   'right': {'feature_index': 4,\n",
       "    'value': -0.2798735903276164,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'feature_index': 10,\n",
       "     'value': -0.2766572325063054,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 6,\n",
       "      'value': -0.2801047519905265,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'M'},\n",
       "      'right': {'majority_label': 'B'}},\n",
       "     'right': {'majority_label': 'M'}},\n",
       "    'right': {'feature_index': 21,\n",
       "     'value': -0.15926894260545973,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 22,\n",
       "      'value': 0.4298025319344636,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'B'},\n",
       "      'right': {'majority_label': 'M'}},\n",
       "     'right': {'majority_label': 'M'}}}}},\n",
       " 'right': {'feature_index': 23,\n",
       "  'value': 4.695437885610253,\n",
       "  'categorical': False,\n",
       "  'majority_label': None,\n",
       "  'left': {'feature_index': 27,\n",
       "   'value': -0.27936923469834535,\n",
       "   'categorical': False,\n",
       "   'majority_label': None,\n",
       "   'left': {'feature_index': 21,\n",
       "    'value': -0.1367202830911969,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'feature_index': 3,\n",
       "     'value': 3.333864762869319,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'majority_label': 'B'},\n",
       "     'right': {'feature_index': 25,\n",
       "      'value': -0.27830759234390734,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'feature_index': 23,\n",
       "       'value': 4.122261250563205,\n",
       "       'categorical': False,\n",
       "       'majority_label': None,\n",
       "       'left': {'majority_label': 'M'},\n",
       "       'right': {'majority_label': 'B'}},\n",
       "      'right': {'majority_label': 'M'}}},\n",
       "    'right': {'feature_index': 1,\n",
       "     'value': -0.162389752327464,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 28,\n",
       "      'value': -0.27843506020541026,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'B'},\n",
       "      'right': {'majority_label': 'M'}},\n",
       "     'right': {'majority_label': 'M'}}},\n",
       "   'right': {'feature_index': 20,\n",
       "    'value': -0.22204573586689047,\n",
       "    'categorical': False,\n",
       "    'majority_label': None,\n",
       "    'left': {'majority_label': 'B'},\n",
       "    'right': {'feature_index': 22,\n",
       "     'value': 0.2447697471814464,\n",
       "     'categorical': False,\n",
       "     'majority_label': None,\n",
       "     'left': {'feature_index': 14,\n",
       "      'value': -0.280284506860173,\n",
       "      'categorical': False,\n",
       "      'majority_label': None,\n",
       "      'left': {'majority_label': 'M'},\n",
       "      'right': {'majority_label': 'B'}},\n",
       "     'right': {'majority_label': 'M'}}}},\n",
       "  'right': {'majority_label': 'M'}}}"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying tree generation\n",
    "tree = build_tree_rf(n_features = 5, X = dataArray[:,1:31], y = dataArray[:,31], sample_weights = uniform_weights,\\\n",
    "                  columns_dict = columns_dict, depth = 1,  max_depth=10, min_samples_leaf=2)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING CT6 SOLUTIONS ## \n",
    "\n",
    "### Inserting max_depth as an argument for cross-validation ###\n",
    "def train_rf(B, max_depth, n_features, X, y,  columns_dict = columns_dict, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Build the decision tree according to the training data.\n",
    "  Args:\n",
    "      B: number of decision trees.\n",
    "      max_depth: maximum depth of the trees\n",
    "      X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "      y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "      sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "  else:\n",
    "      sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "  N = X.shape[0]\n",
    "  training_indices = np.arange(N)\n",
    "  trees = []\n",
    "\n",
    "  for _ in range(B):\n",
    "    sample = np.random.choice(training_indices, N, replace=True)\n",
    "    X_sample = X[sample, :]\n",
    "    y_sample = y[sample]\n",
    "    w_sample = sample_weights[sample]\n",
    "    tree = build_tree_rf(n_features, X_sample, y_sample, w_sample, columns_dict, depth=1,\\\n",
    "                         max_depth=max_depth) ### Maximum depth is no more defaulted to 10 here ###\n",
    "    trees.append(tree)\n",
    "\n",
    "  return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THOSE CT6 SOLUTION FUNCTIONS ## \n",
    "\n",
    "def classify(tree, x):\n",
    "  \"\"\"\n",
    "  Classify a single sample with the fitted decision tree.\n",
    "  Args:\n",
    "      x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "  Returns:\n",
    "      (int): predicted testing sample label.\n",
    "  \"\"\"\n",
    "  if tree['majority_label'] is not None: \n",
    "    return tree['majority_label']\n",
    "\n",
    "  elif tree['categorical']:\n",
    "    if x[tree['feature_index']] == tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)\n",
    "\n",
    "  else:\n",
    "    if x[tree['feature_index']] < tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)\n",
    "\n",
    "\n",
    "def predict_rf(rf, X):\n",
    "  \"\"\"\n",
    "  Predict classification results for X.\n",
    "  Args:\n",
    "      rf: A trained random forest through train_rf function.\n",
    "      X: testing sample features, of shape (N, D).\n",
    "  Returns:\n",
    "      (np.array): predicted testing sample labels, of shape (N,).\n",
    "  \"\"\"\n",
    "\n",
    "  def aggregate(decisions):\n",
    "    count = defaultdict(int)\n",
    "    for decision in decisions:\n",
    "      count[decision] += 1\n",
    "    return max(count, key=count.get)\n",
    "\n",
    "  if len(X.shape) == 1:\n",
    "      return aggregate([classify(tree, X) for tree in rf])\n",
    "  else:\n",
    "      return np.array([aggregate([classify(tree, x) for tree in rf]) for x in X])\n",
    "\n",
    "### Using accuracy ###\n",
    "def rf_score(rf, X_test, y_test):\n",
    "  y_pred = predict_rf(rf, X_test) \n",
    "  return np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the 5-fold cross-validation as previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationRF(dataArray, max_depth, B) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of accuracies of a five-fold cross-validation process\n",
    "    on the random forests model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    max_depth : an integer - the maximum depth of the trees\n",
    "    B : an integer - the number of trees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanAccuracy : a float number - the mean of accuracies of the five-fold cross-validation \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = np.zeros(5) # Container for the accuracies\n",
    "\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        \n",
    "        # Training and Validation\n",
    "        # On training subset\n",
    "        rf = train_rf(B = B, max_depth = max_depth, n_features = 10, X = trainingSubset[:,1:31],\\\n",
    "              y = trainingSubset[:,31]) \n",
    "        # On validation subset\n",
    "        accuracies[i] = rf_score(rf = rf, X_test = validationSubset[:,1:31], y_test = validationSubset[:,31])\n",
    "         \n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestParamsRF(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter max_depth and B by using a five-fold cross-validation\n",
    "    method for the random forests model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of accuracies and the tested max_depth and B\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_maxDepth : a integer - the optimal max-depth hyperparameter\n",
    "    best_B : a integer - the optimal B hyperparameter\n",
    "    maxDepthList : an M-dimensionnal numpy array containing the tested max_depth\n",
    "    BList : an N-dimensionnal numpy array containing the tested B\n",
    "    meanAccuracies : an NxM-dimensionnal numpy array containing the means of accuracies\n",
    "    best_maxDepth_ind : an integer - the optimal max_depth index in maxDepthList\n",
    "    best_B_ind : an integer - the optimal B index in BList\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    BList = np.arange(start = 2, stop = 10 , step = 3) # Scanning B\n",
    "    maxDepthList = np.arange(start = 4, stop = 14, step = 4) # Scanning max_depth\n",
    "    meanAccuracies = np.zeros((BList.shape[0],maxDepthList.shape[0])) # Storing accuracies\n",
    "    \n",
    "    for i, B in enumerate(BList) :\n",
    "        for j, max_depth in enumerate(maxDepthList) :\n",
    "            # Maximizing mean of accuracies\n",
    "            meanAccuracies[i,j] = fiveFoldValidationRF(dataArray, max_depth, B)\n",
    "            print('Cross-validation for B = ' + str(B) + ' and max_depth = ' + str(max_depth) + \\\n",
    "                  ' gives a mean of accuracies = ' + str(meanAccuracies[i,j]))\n",
    "        \n",
    "    best_B_ind, best_maxDepth_ind  = np.where(meanAccuracies == np.amax(meanAccuracies))\n",
    "    # A maximizer of accuracies\n",
    "    best_B_ind = best_B_ind[0]\n",
    "    best_maxDepth_ind = best_maxDepth_ind[0]\n",
    "    \n",
    "    if returnArray :\n",
    "        return BList[best_B_ind], maxDepthList[best_maxDepth_ind], BList, maxDepthList, \\\n",
    "    meanAccuracies, best_B_ind, best_maxDepth_ind\n",
    "    \n",
    "    return BList[best_B_ind], maxDepthList[best_maxDepth_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for B = 5 and max_depth = 8 gives a mean of accuracies = 0.9801169590643275\n",
      "Cross-validation for B = 5 and max_depth = 9 gives a mean of accuracies = 0.9828460038986355\n",
      "Cross-validation for B = 5 and max_depth = 10 gives a mean of accuracies = 0.9840155945419102\n",
      "Cross-validation for B = 5 and max_depth = 11 gives a mean of accuracies = 0.9828460038986353\n",
      "Cross-validation for B = 5 and max_depth = 12 gives a mean of accuracies = 0.9808966861598438\n",
      "Cross-validation for B = 6 and max_depth = 8 gives a mean of accuracies = 0.9824561403508772\n",
      "Cross-validation for B = 6 and max_depth = 9 gives a mean of accuracies = 0.9840155945419102\n",
      "Cross-validation for B = 6 and max_depth = 10 gives a mean of accuracies = 0.9847953216374268\n",
      "Cross-validation for B = 6 and max_depth = 11 gives a mean of accuracies = 0.983625730994152\n",
      "Cross-validation for B = 6 and max_depth = 12 gives a mean of accuracies = 0.9812865497076023\n",
      "Cross-validation for B = 7 and max_depth = 8 gives a mean of accuracies = 0.9832358674463937\n",
      "Cross-validation for B = 7 and max_depth = 9 gives a mean of accuracies = 0.9793372319688108\n",
      "Cross-validation for B = 7 and max_depth = 10 gives a mean of accuracies = 0.9847953216374268\n",
      "Cross-validation for B = 7 and max_depth = 11 gives a mean of accuracies = 0.983625730994152\n",
      "Cross-validation for B = 7 and max_depth = 12 gives a mean of accuracies = 0.9836257309941521\n",
      "Cross-validation for B = 8 and max_depth = 8 gives a mean of accuracies = 0.9844054580896685\n",
      "Cross-validation for B = 8 and max_depth = 9 gives a mean of accuracies = 0.9875243664717349\n",
      "Cross-validation for B = 8 and max_depth = 10 gives a mean of accuracies = 0.9859649122807017\n",
      "Cross-validation for B = 8 and max_depth = 11 gives a mean of accuracies = 0.9867446393762183\n",
      "Cross-validation for B = 8 and max_depth = 12 gives a mean of accuracies = 0.9840155945419102\n"
     ]
    }
   ],
   "source": [
    "# Computing the optimal parameters \n",
    "bestB, bestMaxDepth, BList, maxDepthList, \\\n",
    "    meanAccuracies, best_B_ind, best_maxDepth_ind = chooseBestParamsRF(dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Very long wall time - Saving the variables here ###\n",
    "\n",
    "# Cross-validation for B = 5 and max_depth = 8 gives a mean of accuracies = 0.9801169590643275\n",
    "# Cross-validation for B = 5 and max_depth = 9 gives a mean of accuracies = 0.9828460038986355\n",
    "# Cross-validation for B = 5 and max_depth = 10 gives a mean of accuracies = 0.9840155945419102\n",
    "# Cross-validation for B = 5 and max_depth = 11 gives a mean of accuracies = 0.9828460038986353\n",
    "# Cross-validation for B = 5 and max_depth = 12 gives a mean of accuracies = 0.9808966861598438\n",
    "# Cross-validation for B = 6 and max_depth = 8 gives a mean of accuracies = 0.9824561403508772\n",
    "# Cross-validation for B = 6 and max_depth = 9 gives a mean of accuracies = 0.9840155945419102\n",
    "# Cross-validation for B = 6 and max_depth = 10 gives a mean of accuracies = 0.9847953216374268\n",
    "# Cross-validation for B = 6 and max_depth = 11 gives a mean of accuracies = 0.983625730994152\n",
    "# Cross-validation for B = 6 and max_depth = 12 gives a mean of accuracies = 0.9812865497076023\n",
    "# Cross-validation for B = 7 and max_depth = 8 gives a mean of accuracies = 0.9832358674463937\n",
    "# Cross-validation for B = 7 and max_depth = 9 gives a mean of accuracies = 0.9793372319688108\n",
    "# Cross-validation for B = 7 and max_depth = 10 gives a mean of accuracies = 0.9847953216374268\n",
    "# Cross-validation for B = 7 and max_depth = 11 gives a mean of accuracies = 0.983625730994152\n",
    "# Cross-validation for B = 7 and max_depth = 12 gives a mean of accuracies = 0.9836257309941521\n",
    "# Cross-validation for B = 8 and max_depth = 8 gives a mean of accuracies = 0.9844054580896685\n",
    "# Cross-validation for B = 8 and max_depth = 9 gives a mean of accuracies = 0.9875243664717349\n",
    "# Cross-validation for B = 8 and max_depth = 10 gives a mean of accuracies = 0.9859649122807017\n",
    "# Cross-validation for B = 8 and max_depth = 11 gives a mean of accuracies = 0.9867446393762183\n",
    "# Cross-validation for B = 8 and max_depth = 12 gives a mean of accuracies = 0.9840155945419102\n",
    "\n",
    "# meanAccuracies = np.array([[0.98011696, 0.982846  , 0.98401559, 0.982846  , 0.98089669],\n",
    "      # [0.98245614, 0.98401559, 0.98479532, 0.98362573, 0.98128655],\n",
    "      # [0.98323587, 0.97933723, 0.98479532, 0.98362573, 0.98362573],\n",
    "      # [0.98440546, 0.98752437, 0.98596491, 0.98674464, 0.98401559]])\n",
    "    \n",
    "# BList = np.array([5, 6, 7, 8])\n",
    "\n",
    "# maxDepthList = np.array([ 8,  9, 10, 11, 12])\n",
    "\n",
    "# bestB, bestMaxDepth = (8, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Performance assessment \n",
    "\n",
    "We compute accuracy scores on the training and testing set using our optimal hyperparameters $B$ and $\\texttt{max}$_$\\texttt{depth}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re importing and converting \n",
    "dataArray = df_tumour.to_numpy()\n",
    "testingDataArray = df_tumour_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random forest using the previous helper functions \n",
    "n_features = dataArray[:,1:31].shape[1] // 3\n",
    "rf = train_rf(B = bestB, max_depth = bestMaxDepth, n_features = n_features, X = dataArray[:,1:31],\\\n",
    "              y = dataArray[:,31], columns_dict = columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy computed on full training set =  0.9980514419329696\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy score on full training set\n",
    "RFtrainingScore = rf_score(rf = rf, X_test = dataArray[:,1:31], y_test = dataArray[:,31])\n",
    "print('Accuracy computed on full training set = ',RFtrainingScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy computed on unseen testing set =  0.985\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy score on unseen testing set \n",
    "RFtestingScore = rf_score(rf = rf, X_test = testingDataArray[:,1:31], y_test = testingDataArray[:,31])\n",
    "print('Accuracy computed on unseen testing set = ',RFtestingScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now modify the $\\texttt{rf}$_$\\texttt{score}$ function in order to retreive the numbers of true positives, false positives, true negatives and false negatives. This will allow us to obtain the confusion matrix of the given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatrixRF(rf, X_test, y_test) :\n",
    "    \"\"\"\n",
    "    This function computes the confusion matrix for the given random forest on the given testing set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rf : a list of dictionnaries - A trained random forest through train_rf function\n",
    "    X_test : an Nxp-dimensionnal numpy array - the predictors\n",
    "    y_test : an N-dimensionnal numpy array - the targets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : an integer - the number of true positives\n",
    "    TN : an integer - the number of true negatives\n",
    "    FP : an integer - the number of false positives\n",
    "    FN : an integer - the number of false negatives\n",
    "    confusionMatrix : an 2x2-dimensionnal numpy array - the confusion matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    N = X_test.shape[0]\n",
    "    \n",
    "    TP = 0 # Positive is 'M' in our case\n",
    "    TN = 0 # Negative is 'B' in our case\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    y_pred = predict_rf(rf, X_test)\n",
    "    predictionsCorrectness = (y_pred==y_test)\n",
    "    \n",
    "    for i in range(N) :\n",
    "        \n",
    "        if predictionsCorrectness[i] and y_test[i] == 'M' :\n",
    "            TP += 1\n",
    "        elif predictionsCorrectness[i] and y_test[i] == 'B' :\n",
    "            TN += 1\n",
    "        elif not predictionsCorrectness[i] and y_test[i] == 'M' :\n",
    "            FN += 1\n",
    "        elif not predictionsCorrectness[i] and y_test[i] == 'B' :\n",
    "            FP += 1\n",
    "    \n",
    "    confusionMatrix = np.array([[TP, FP],[FN, TN]])\n",
    "    \n",
    "    return TP, TN, FP, FN, confusionMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 419    0]\n",
      " [   5 2142]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix on full training set \n",
    "TP, TN, FP, FN, confusionMatrix = \\\n",
    "getConfusionMatrixRF(rf = rf, X_test = dataArray[:,1:31], y_test = dataArray[:,31])\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 70   0]\n",
      " [  3 127]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix on unseen testing set \n",
    "TP, TN, FP, FN, confusionMatrix = \\\n",
    "getConfusionMatrixRF(rf = rf, X_test = testingDataArray[:,1:31], y_test = testingDataArray[:,31])\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a series of score functions based on the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(TP,FN) :\n",
    "    \"\"\"\n",
    "    This function computes the recall based on the number of true positives and false negatives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : an integer - the number of true positives\n",
    "    FN : an integer - the number of false negatives \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    recall : a float number - the value of recall\n",
    "    \"\"\"\n",
    "    \n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def precision(TP,FP) :\n",
    "    \"\"\"\n",
    "    This function computes the precision based on the number of true positives and false positives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : an integer - the number of true positives\n",
    "    FP : an integer - the number of false positives \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision : a float number - the value of precision\n",
    "    \"\"\"\n",
    "    \n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def Fscore(TP,FP,FN) :\n",
    "    \"\"\"\n",
    "    This function computes the F-score based on the number of true positives, false positives and false negatives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : an integer - the number of true positives\n",
    "    FP : an integer - the number of false positives \n",
    "    FN : an integer - the number of false negatives \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : a float number - the value of the F-score\n",
    "    \"\"\"\n",
    "    \n",
    "    rec = recall(TP,FN)\n",
    "    prec = precision(TP,FP)\n",
    "    \n",
    "    \n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the performance of the model on the training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy computed on full training set =  0.9980514419329696\n",
      "Recall computed on full training set =  0.9882075471698113\n",
      "Precision computed on full training set =  1.0\n",
      "F-score computed on full training set =  0.9940688018979834\n"
     ]
    }
   ],
   "source": [
    "# Computing scores on full training set\n",
    "RFtrainingAccuracy = rf_score(rf = rf, X_test = dataArray[:,1:31], y_test = dataArray[:,31])\n",
    "TP, TN, FP, FN, confusionMatrix = \\\n",
    "getConfusionMatrixRF(rf = rf, X_test = dataArray[:,1:31], y_test = dataArray[:,31])\n",
    "RFtrainingRecall = recall(TP,FN)\n",
    "RFtrainingPrecision = precision(TP,FP)\n",
    "RFtrainingFscore = Fscore(TP,FP,FN)\n",
    "print('Accuracy computed on full training set = ',RFtrainingAccuracy)\n",
    "print('Recall computed on full training set = ',RFtrainingRecall)\n",
    "print('Precision computed on full training set = ',RFtrainingPrecision)\n",
    "print('F-score computed on full training set = ',RFtrainingFscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy computed on unseen testing set =  0.985\n",
      "Recall computed on unseen testing set =  0.958904109589041\n",
      "Precision computed on unseen testing set =  1.0\n",
      "F-score computed on unseen testing set =  0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Computing scores on unseen testing set\n",
    "RFtestingAccuracy = rf_score(rf = rf, X_test = testingDataArray[:,1:31], y_test = testingDataArray[:,31])\n",
    "TP, TN, FP, FN, confusionMatrix = \\\n",
    "getConfusionMatrixRF(rf = rf, X_test = testingDataArray[:,1:31], y_test = testingDataArray[:,31])\n",
    "RFtestingRecall = recall(TP,FN)\n",
    "RFtestingPrecision = precision(TP,FP)\n",
    "RFtestingFscore = Fscore(TP,FP,FN)\n",
    "print('Accuracy computed on unseen testing set = ',RFtestingAccuracy)\n",
    "print('Recall computed on unseen testing set = ',RFtestingRecall)\n",
    "print('Precision computed on unseen testing set = ',RFtestingPrecision)\n",
    "print('F-score computed on unseen testing set = ',RFtestingFscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First notice that we don't have any false positives on the training data nor on the testing data, leading to a precision of 1. On the other hand, the model induces a small number of false negatives both on the training and testing data. The number of false negatives is rather low with respect to the number of samples predicted. This is a slighlty higher proportion on the testing set (recall $\\approx $ 0.95) than on the training set (recall $\\approx$ 0.99). Notice that in this medical context, unfortunately, false positives are more acceptable than false negatives, as a practitioner-based cross-checking may be triggered for all (unfrequent) positives.\n",
    "\n",
    "Unsurprisingly, the F-score (harmonic mean of precision and recall which penalizes more extreme values) is lower than the accuracy on both data. However, the model's predictions are overall rather high on both sets (more than 99% in accuracy and more than 98% in F-score). The testing set accuracy scores (accuracy and F-score) stay fairly close to the ones of the training set, displaying that the number of trees chosen is sufficient to avoid any significant overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfN9uz1qy1L4"
   },
   "source": [
    "<a name=\"q23\"></a>\n",
    "\n",
    "## 2.3 Support vector machine (SVM) [^](#outline)\n",
    "\n",
    "We now use a soft margin support vector machine method to perform the tumour samples classfication task. We find an optimal hyperplane best separating the two considered classes in the input space. We further use this hyperplane to predict the class of any new unseen entry, depending on the side on which said input falls. \n",
    "\n",
    "In this setting, we are seeking to minimize : $\\min_{\\mathbf{\\omega}} \\frac{1}{2}||\\mathbf{\\omega}||^2 + \\frac{\\lambda}{N} \\sum_i^N \\zeta^{(i)} \\text{ with constrain } 1-y^{(i)}(\\mathbf{\\omega}\\cdot\\mathbf{x}^{(i)}+ b)\\leq\\zeta^{(i)}$. We have that $\\mathbf{w}$ is the vector normal to the hyperplane, and $\\zeta^{(i)}= \\max(0,1-y^{(i)}(\\mathbf{\\omega}\\cdot\\mathbf{x}^{(i)}+ b))$ is a penalty term incurred for training data points on the wrong side of the plane (by considering their given class).\n",
    "\n",
    "\n",
    "As previously, in the following, we build several helper functions to perform this task.\n",
    "\n",
    "### 2.3.1 Training \n",
    "\n",
    "We use the same 5-fold cross-validation process to determine an optimal $\\textit{hardness}$ hyperparameter $\\lambda$. We use accuracy to measure the performance of the model.\n",
    "\n",
    "We still heavily $\\textbf{rely on the weekly coding tasks solutions}$, and further $\\textbf{modify them to suit our current problem}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'B' 'B' ... 'B' 'B' 'B']\n",
      "[-1.0 -1.0 -1.0 ... -1.0 -1.0 -1.0]\n"
     ]
    }
   ],
   "source": [
    "# In the weekly Coding Task solution functions, you may find my custom comments in the form of ### _____ ###\n",
    "\n",
    "# Serializing the classes : 'M' becomes 1.0 and 'B' becomes -1.0\n",
    "dataArray = df_tumour.to_numpy()\n",
    "print(dataArray[:,31])\n",
    "serialMap = {'M': 1.0, 'B': -1.0}\n",
    "length = len(dataArray[:,31])\n",
    "dataArray[:,31] = [serialMap[dataArray[i,31]] for i in range(length)]\n",
    "print(dataArray[:,31])\n",
    "\n",
    "# Standardization\n",
    "dataArray[:,1:31] = (dataArray[:,1:31] - np.mean(dataArray[:,1:31])) / np.std(dataArray[:,1:31])\n",
    "\n",
    "# Augmenting dataArray to fit intercept \n",
    "dataArray = np.insert(dataArray, 31, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "def compute_cost(w, X, y, regul_strength=1e5):\n",
    "    n = X.shape[0]\n",
    "    distances = 1 - y * (X @ w)  \n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge = regul_strength * distances.mean() \n",
    "\n",
    "    # calculate cost\n",
    "    return 0.5 * np.dot(w, w) + hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "# calculate gradient of cost\n",
    "def calculate_cost_gradient(w, X_batch, y_batch, regul_strength=1e5):\n",
    "    # if only one example is passed\n",
    "    y_batch = np.asarray([y_batch])\n",
    "    X_batch = np.asarray([X_batch])  # gives multidimensional array\n",
    "\n",
    "    distance = 1 - (y_batch * (X_batch @ w))\n",
    "    dw = np.zeros(len(w), dtype = float)\n",
    "\n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d)==0:\n",
    "            di = w\n",
    "            di = di.astype(float)\n",
    "        else:\n",
    "            di = w - (regul_strength * y_batch[ind] * X_batch[ind])\n",
    "            di = di.astype(float)\n",
    "        dw += di\n",
    "\n",
    "    return dw/len(y_batch)  # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "def sgd(X, y, batch_size=32, max_iterations=2000, stop_criterion=0.01, \\\n",
    "        learning_rate=1e-5, regul_strength=1e5, print_outcome=False):\n",
    "    # initialise zero weights\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    nth = 0\n",
    "    # initialise starting cost as infinity\n",
    "    prev_cost = np.inf\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    indices = np.arange(len(y))\n",
    "    for iteration in range(1, max_iterations):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        np.random.shuffle(indices)\n",
    "        batch_idx = indices[:batch_size]\n",
    "        X_b, y_b = X[batch_idx], y[batch_idx]\n",
    "        for xi, yi in zip(X_b, y_b):\n",
    "            ascent = calculate_cost_gradient(weights, xi, yi, regul_strength) \n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^n'th iteration\n",
    "        if iteration==2**nth or iteration==max_iterations-1:\n",
    "            # compute cost\n",
    "            cost = compute_cost(weights, X, y, regul_strength) \n",
    "            if print_outcome:\n",
    "                print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "            # stop criterion\n",
    "            if abs(prev_cost - cost) < stop_criterion * prev_cost:\n",
    "                return weights\n",
    "              \n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 681.9472616134296\n",
      "Iteration is: 2, Cost is: 558.7339312641124\n",
      "Iteration is: 4, Cost is: 333.9004692363865\n",
      "Iteration is: 8, Cost is: 220.80461451918143\n",
      "Iteration is: 16, Cost is: 171.06061846898717\n",
      "Iteration is: 32, Cost is: 223.0781753506032\n",
      "Iteration is: 64, Cost is: 143.53314127015454\n",
      "Iteration is: 128, Cost is: 125.51534903988099\n",
      "Iteration is: 256, Cost is: 133.91641291653514\n",
      "Iteration is: 512, Cost is: 126.7644988261685\n",
      "Iteration is: 1024, Cost is: 135.04624032986868\n",
      "Iteration is: 1999, Cost is: 140.28398187510544\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "w = sgd(X = dataArray[:,1:32], y = dataArray[:,32], regul_strength=1e3, print_outcome=True)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "### Using accuracy ###\n",
    "def score(w, X, y):\n",
    "    y_preds = np.sign(X @ w)\n",
    "    return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the 5-fold cross-validation process to find an optimal $\\lambda$ regarding accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationSVM(dataArray, lbda) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of accuracies of a five-fold cross-validation method\n",
    "    on the SVM model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    lbda : a float - the hardness hyperparameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanAccuracy : a float number - the mean of accuracies of the five-fold cross-validation \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = np.zeros(5) # Container for the accuracies\n",
    "\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        \n",
    "        # Training and Validation\n",
    "        # On training subset\n",
    "        w = sgd(X = trainingSubset[:,1:32], y = trainingSubset[:,32], regul_strength = lbda)\n",
    "        # On validation subset\n",
    "        accuracies[i] = score(w = w, X = validationSubset[:,1:32], y = validationSubset[:,32])\n",
    "         \n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestLambdaSVM(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter lambda by using a five-fold cross-validation\n",
    "    method for the soft margin linear SVM method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of accuracies and the tested lambdas \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_lambda : a float number - the optimal hyperparameter\n",
    "    lambdas : an M-dimensionnal numpy array containing the tested lambdas\n",
    "    lbdaAcc : an M-dimensionnal numpy array containing the means of accuracies\n",
    "    best_lambda_ind : an integer - the optimal lambda index in lambdas\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    lambdas = np.linspace(start = 0.001, stop = 1000, num = 100) # Scanning lambdas\n",
    "    accuracies = np.zeros(lambdas.shape[0])\n",
    "    \n",
    "    for i, lbda in enumerate(lambdas) :\n",
    "        # Maximizing mean of accuracies\n",
    "        accuracies[i] = fiveFoldValidationSVM(dataArray = dataArray, lbda = lbda)\n",
    "        if i%15 == 0 : # Prints from time to time\n",
    "            print('Cross-validation for lambda = ' + str(lbda) + ' gives a mean of accuracies = '\\\n",
    "                  + str(accuracies[i]))\n",
    "        \n",
    "    best_lambda_ind = np.argmax(accuracies) # A maximizer of means of accuracies\n",
    "    \n",
    "    if returnArray :\n",
    "        return lambdas[best_lambda_ind], lambdas, accuracies, best_lambda_ind\n",
    "    \n",
    "    return lambdas[best_lambda_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for lambda = 0.001 gives a mean of accuracies = 0.8346978557504873\n",
      "Cross-validation for lambda = 151.51600000000002 gives a mean of accuracies = 0.953606237816764\n",
      "Cross-validation for lambda = 303.031 gives a mean of accuracies = 0.9247563352826511\n",
      "Cross-validation for lambda = 454.546 gives a mean of accuracies = 0.9563352826510721\n",
      "Cross-validation for lambda = 606.061 gives a mean of accuracies = 0.935672514619883\n",
      "Cross-validation for lambda = 757.576 gives a mean of accuracies = 0.9516569200779728\n",
      "Cross-validation for lambda = 909.091 gives a mean of accuracies = 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "# Computing the optimal hyperparameter based on accuracy\n",
    "bestLambdaSVM, lambdas, accuracies, best_lambda_ind = \\\n",
    "chooseBestLambdaSVM(dataArray = dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal λ = 747.475\n",
      "Maximum mean of accuracies = 0.9594541910331383\n"
     ]
    }
   ],
   "source": [
    "# Optimal lambda hyperparameter for SVM\n",
    "print('Optimal λ = ' + str(bestLambdaSVM))\n",
    "# Corresponding mean of accuracies\n",
    "print('Maximum mean of accuracies = ' + str(accuracies[best_lambda_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9d15bc4c0>]"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting mean of accuracies against different lambdas : SVM\n",
    "plt.figure()\n",
    "title = 'Mean of accuracies computed using five-fold cross-validation against hyperparameter $\\lambda$ \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Mean of accuracies')\n",
    "plt.plot(lambdas,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\lambda$ balances the trade off that appears between having the largest minimum margin and having the minimum number of misclassified samples through the set. Namely, higher $\\lambda$ is, higher we penalize misclassification and thus more we request a better fitting hyperplane. However, to some extent, having a better fitting of the hyperplane disables the possibility of having large margins between the two classes values, and further weakens the possibility of having a firm separation between the data. Having $\\lambda$ small may increase said margins, however it is done at the price of some outlying misclassifications.\n",
    "\n",
    "In this case, it seems that no suitable large margin can be found may $\\lambda$ be small. Moreover, the accuracy is saturating for $\\lambda > 75$, not increasing the performance further. \n",
    "\n",
    "### 2.3.2 Receiver operating characteristic on testing set\n",
    "\n",
    "We now compute the performance of the model over the testing set for various hyperparameters. We display the results as a receiver operating characteristic curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatrixSVM(w, X_test, y_test) :\n",
    "    \"\"\"\n",
    "    This function computes the confusion matrix for the given SVM on the given testing set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : an p-dimensionnal numpy array - the vector defining the hyperplane \n",
    "    X_test : an Nxp-dimensionnal numpy array - the predictors\n",
    "    y_test : an N-dimensionnal numpy array - the targets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : an integer - the number of true positives\n",
    "    TN : an integer - the number of true negatives\n",
    "    FP : an integer - the number of false positives\n",
    "    FN : an integer - the number of false negatives\n",
    "    confusionMatrix : an 2x2-dimensionnal numpy array - the confusion matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    N = X_test.shape[0]\n",
    "    \n",
    "    \n",
    "    TP = 0 # Positive is 'M' in our case\n",
    "    TN = 0 # Negative is 'B' in our case\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    y_pred = np.sign(X_test @ w)\n",
    "    predictionsCorrectness = (y_pred==y_test)\n",
    "    \n",
    "    for i in range(N) :\n",
    "        \n",
    "        if predictionsCorrectness[i] and y_test[i] == 1.0 :\n",
    "            TP += 1\n",
    "        elif predictionsCorrectness[i] and y_test[i] == -1.0 :\n",
    "            TN += 1\n",
    "        elif not predictionsCorrectness[i] and y_test[i] == 1.0 :\n",
    "            FN += 1\n",
    "        elif not predictionsCorrectness[i] and y_test[i] == -1.0 :\n",
    "            FP+= 1\n",
    "    \n",
    "    confusionMatrix = np.array([[TP, FP],[FN, TN]])\n",
    "    \n",
    "    return TP, TN, FP, FN, confusionMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the classes : 'M' becomes 1.0 and 'B' becomes -1.0\n",
    "testingDataArray = df_tumour_test.to_numpy()\n",
    "serialMap = {'M': 1.0, 'B': -1.0}\n",
    "length = len(testingDataArray[:,31])\n",
    "testingDataArray[:,31] = [serialMap[testingDataArray[i,31]] for i in range(length)]\n",
    "\n",
    "# Standardization\n",
    "testingDataArray[:,1:31] = (testingDataArray[:,1:31] - np.mean(testingDataArray[:,1:31])) \\\n",
    "/ np.std(testingDataArray[:,1:31])\n",
    "\n",
    "# Augmenting dataArray to fit intercept \n",
    "testingDataArray = np.insert(testingDataArray, 31, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 419.4174467053221\n",
      "Iteration is: 2, Cost is: 381.9606339064277\n",
      "Iteration is: 4, Cost is: 340.90094574857045\n",
      "Iteration is: 8, Cost is: 174.86227086273485\n",
      "Iteration is: 16, Cost is: 152.24745725178806\n",
      "Iteration is: 32, Cost is: 127.83362705002139\n",
      "Iteration is: 64, Cost is: 99.73397377931684\n",
      "Iteration is: 128, Cost is: 108.32983304323041\n",
      "Iteration is: 256, Cost is: 95.34086415231133\n",
      "Iteration is: 512, Cost is: 104.78284232554064\n",
      "Iteration is: 1024, Cost is: 95.09959043195151\n",
      "Iteration is: 1999, Cost is: 96.49020235179567\n"
     ]
    }
   ],
   "source": [
    "# Training the model with optimal lambda \n",
    "wBest = sgd(X = dataArray[:,1:32], y = dataArray[:,32], regul_strength = bestLambdaSVM, print_outcome=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48   0]\n",
      " [ 25 127]]\n"
     ]
    }
   ],
   "source": [
    "# Computing confusion matrix on testing set\n",
    "TP, TN, FP, FN, confusionMatrix = getConfusionMatrixSVM(w = wBest, X_test = testingDataArray[:,1:32], \\\n",
    "                                                        y_test = testingDataArray[:,32])\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_SVM(X_train, y_train, X_test, y_test) :\n",
    "    \"\"\"\n",
    "    This function computes the receiver operating characteristic curve values of the SVM model. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : an Nxp-dimensionnal numpy array - the train predictors\n",
    "    y_train : an N-dimensionnal numpy array - the train targets\n",
    "    X_test : an Nxp-dimensionnal numpy array - the test predictors\n",
    "    y_test : an N-dimensionnal numpy array - the test targets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    FPR : an M-dimensionnal numpy array - the false positive rates \n",
    "    TPR : an M-dimensionnal numpy array- the true positive rates\n",
    "    \"\"\"\n",
    "    \n",
    "    lambdas = np.linspace(start = 0.001, stop = 50000, num = 300) # Scanning lambdas\n",
    "    \n",
    "    FPR = np.zeros(lambdas.shape[0])\n",
    "    TPR = np.zeros(lambdas.shape[0])\n",
    "    \n",
    "    for i, lbda in enumerate(lambdas) :\n",
    "        \n",
    "        # Training\n",
    "        w = sgd(X = X_train, y = y_train, regul_strength = lbda)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        TP, TN, FP, FN, _ = getConfusionMatrixSVM(w = w, X_test = X_test, \\\n",
    "                                                        y_test = y_test)\n",
    "        try : # Avoid divisions by zero\n",
    "            FPR[i] = FP / (FP + TN) # False positive rate\n",
    "            TPR[i] = TP / (TP + FN) # True positive rate\n",
    "        except :\n",
    "            FPR[i] = -1.0\n",
    "            TPR[i] = -1.0\n",
    "        \n",
    "    FPR = FPR[FPR != -1.0] # Removing \n",
    "    TPR = TPR[TPR != -1.0]\n",
    "        \n",
    "    return FPR, TPR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing best hyperparameter ROC point (FPR,TPR)\n",
    "        \n",
    "# Confusion matrix\n",
    "bestTP, bestTN, bestFP, bestFN, _ = getConfusionMatrixSVM(w = wBest, X_test = testingDataArray[:,1:32], \\\n",
    "                                                        y_test = testingDataArray[:,32])\n",
    "\n",
    "bestLbdaFPR = bestFP / (bestFP + bestTN) # False positive rate\n",
    "bestLbdaTPR = bestTP / (bestTP + bestFN) # True positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing ROC\n",
    "FPR, TPR = ROC_SVM(X_train = dataArray[:,1:32], y_train = dataArray[:,32], \\\n",
    "        X_test = testingDataArray[:,1:32], y_test = testingDataArray[:,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc9d0c28730>"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting ROC\n",
    "plt.figure()\n",
    "title = 'Receiver operating characteristic curve of the SVM model \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim([-0.01,1.1])\n",
    "plt.ylim([-0.01,1.1])\n",
    "plt.plot([i for i in range(2)], [i for i in range(2)], color = 'green', ls = '--')\n",
    "plt.scatter(FPR, TPR, s=5)\n",
    "plt.scatter(bestLbdaFPR, bestLbdaTPR, s=25, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display in red the pair (FPR,TPR) computed using our optimal hyperparameter. We are ideally seeking for the smallest false positive rate along with the greatest true positive rate possible. For our optimal $\\lambda$, we have a false positive rate of 0, displaying the fact that no false positives arise from our predictions. Although the computed hyperparameter does not maximize the true positive rate for this given false positive rate level, the performance of the model remains fairly high, being substantially better than a uniform random selection process symbolized by the dashed line.  \n",
    "\n",
    "### 2.3.3 Using balanced data\n",
    "\n",
    "We now retrain our model on a balanced dataset and further compare its performance with our previously trained SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>627</td>\n",
       "      <td>12.087899</td>\n",
       "      <td>16.205290</td>\n",
       "      <td>87.017535</td>\n",
       "      <td>547.339343</td>\n",
       "      <td>0.086909</td>\n",
       "      <td>0.081002</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>0.167734</td>\n",
       "      <td>...</td>\n",
       "      <td>19.771306</td>\n",
       "      <td>92.575634</td>\n",
       "      <td>621.673267</td>\n",
       "      <td>0.136089</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>0.328388</td>\n",
       "      <td>0.086810</td>\n",
       "      <td>0.294363</td>\n",
       "      <td>0.077496</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>14.999447</td>\n",
       "      <td>23.003345</td>\n",
       "      <td>90.973309</td>\n",
       "      <td>524.088231</td>\n",
       "      <td>0.089497</td>\n",
       "      <td>0.099329</td>\n",
       "      <td>0.053181</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>...</td>\n",
       "      <td>27.307626</td>\n",
       "      <td>115.205086</td>\n",
       "      <td>1082.041301</td>\n",
       "      <td>0.132672</td>\n",
       "      <td>0.320725</td>\n",
       "      <td>0.350721</td>\n",
       "      <td>0.143251</td>\n",
       "      <td>0.301848</td>\n",
       "      <td>0.098601</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>390</td>\n",
       "      <td>22.073516</td>\n",
       "      <td>17.604050</td>\n",
       "      <td>138.498180</td>\n",
       "      <td>1425.432345</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.164388</td>\n",
       "      <td>...</td>\n",
       "      <td>25.125070</td>\n",
       "      <td>196.732308</td>\n",
       "      <td>3150.579304</td>\n",
       "      <td>0.134453</td>\n",
       "      <td>0.163258</td>\n",
       "      <td>0.293642</td>\n",
       "      <td>0.194618</td>\n",
       "      <td>0.253306</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>19.262679</td>\n",
       "      <td>19.147128</td>\n",
       "      <td>124.310868</td>\n",
       "      <td>1155.136280</td>\n",
       "      <td>0.116959</td>\n",
       "      <td>0.170659</td>\n",
       "      <td>0.249389</td>\n",
       "      <td>0.109277</td>\n",
       "      <td>0.204006</td>\n",
       "      <td>...</td>\n",
       "      <td>27.899546</td>\n",
       "      <td>173.840392</td>\n",
       "      <td>2327.501525</td>\n",
       "      <td>0.145617</td>\n",
       "      <td>0.356979</td>\n",
       "      <td>0.660332</td>\n",
       "      <td>0.226506</td>\n",
       "      <td>0.322246</td>\n",
       "      <td>0.091355</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>714</td>\n",
       "      <td>12.458203</td>\n",
       "      <td>21.399172</td>\n",
       "      <td>77.306696</td>\n",
       "      <td>454.998813</td>\n",
       "      <td>0.100990</td>\n",
       "      <td>0.097003</td>\n",
       "      <td>0.056405</td>\n",
       "      <td>0.030843</td>\n",
       "      <td>0.150244</td>\n",
       "      <td>...</td>\n",
       "      <td>29.517417</td>\n",
       "      <td>88.260510</td>\n",
       "      <td>555.457650</td>\n",
       "      <td>0.142491</td>\n",
       "      <td>0.233752</td>\n",
       "      <td>0.132008</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.235747</td>\n",
       "      <td>0.089882</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  n1_radius  n1_texture  n1_perimeter      n1_area  \\\n",
       "627         627  12.087899   16.205290     87.017535   547.339343   \n",
       "159         159  14.999447   23.003345     90.973309   524.088231   \n",
       "390         390  22.073516   17.604050    138.498180  1425.432345   \n",
       "52           52  19.262679   19.147128    124.310868  1155.136280   \n",
       "714         714  12.458203   21.399172     77.306696   454.998813   \n",
       "\n",
       "     n1_smoothness  n1_compactness  n1_concavity  n1_concave_points  \\\n",
       "627       0.086909        0.081002      0.051634           0.029736   \n",
       "159       0.089497        0.099329      0.053181           0.033369   \n",
       "390       0.091908        0.084731      0.098110           0.073845   \n",
       "52        0.116959        0.170659      0.249389           0.109277   \n",
       "714       0.100990        0.097003      0.056405           0.030843   \n",
       "\n",
       "     n1_symmetry  ...  n3_texture  n3_perimeter      n3_area  n3_smoothness  \\\n",
       "627     0.167734  ...   19.771306     92.575634   621.673267       0.136089   \n",
       "159     0.170500  ...   27.307626    115.205086  1082.041301       0.132672   \n",
       "390     0.164388  ...   25.125070    196.732308  3150.579304       0.134453   \n",
       "52      0.204006  ...   27.899546    173.840392  2327.501525       0.145617   \n",
       "714     0.150244  ...   29.517417     88.260510   555.457650       0.142491   \n",
       "\n",
       "     n3_compactness  n3_concavity  n3_concave_points  n3_symmetry  \\\n",
       "627        0.263930      0.328388           0.086810     0.294363   \n",
       "159        0.320725      0.350721           0.143251     0.301848   \n",
       "390        0.163258      0.293642           0.194618     0.253306   \n",
       "52         0.356979      0.660332           0.226506     0.322246   \n",
       "714        0.233752      0.132008           0.080336     0.235747   \n",
       "\n",
       "     n3_fractal_dimension  DIAGNOSIS  \n",
       "627              0.077496          B  \n",
       "159              0.098601          M  \n",
       "390              0.064516          M  \n",
       "52               0.091355          M  \n",
       "714              0.089882          B  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load balanced tumour data into a pandas dataframe \n",
    "df_tumour_bal = pd.read_csv(\"tumour_samples_bal.csv\")\n",
    "# Shuffling\n",
    "df_tumour_bal = df_tumour_bal.sample(frac=1)\n",
    "# Displaying the first five rows\n",
    "df_tumour_bal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of M in balanced dataset = 400\n",
      "Number of B in balanced dataset = 400\n"
     ]
    }
   ],
   "source": [
    "# Displaying the structure of this 'balanced' dataset\n",
    "balDataArray = df_tumour_bal.to_numpy()\n",
    "print('Number of M in balanced dataset = ' + str(list(balDataArray[:,31]).count('M')))\n",
    "print('Number of B in balanced dataset = ' + str(list(balDataArray[:,31]).count('B')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the classes : 'M' becomes 1.0 and 'B' becomes -1.0\n",
    "serialMap = {'M': 1.0, 'B': -1.0}\n",
    "length = len(balDataArray[:,31])\n",
    "balDataArray[:,31] = [serialMap[balDataArray[i,31]] for i in range(length)]\n",
    "\n",
    "# Standardization\n",
    "balDataArray[:,1:31] = (balDataArray[:,1:31] - np.mean(balDataArray[:,1:31])) / np.std(balDataArray[:,1:31])\n",
    "\n",
    "# Augmenting dataArray to fit intercept \n",
    "balDataArray = np.insert(balDataArray, 31, 1, axis=1)\n",
    "\n",
    "# Adding a row at the end to fit our fiveFoldValidationSVM function which drops the last row \n",
    "# In order for the set length to be dividable by 5, which is the case here without dropping last row\n",
    "balDataArrayFF = np.insert(balDataArray, 800, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for lambda = 0.001 gives a mean of accuracies = 0.5\n",
      "Cross-validation for lambda = 151.51600000000002 gives a mean of accuracies = 0.67\n",
      "Cross-validation for lambda = 303.031 gives a mean of accuracies = 0.7537499999999999\n",
      "Cross-validation for lambda = 454.546 gives a mean of accuracies = 0.8887499999999999\n",
      "Cross-validation for lambda = 606.061 gives a mean of accuracies = 0.88375\n",
      "Cross-validation for lambda = 757.576 gives a mean of accuracies = 0.81125\n",
      "Cross-validation for lambda = 909.091 gives a mean of accuracies = 0.8625\n"
     ]
    }
   ],
   "source": [
    "# Computing the optimal hyperparameter based on accuracy for balanced data\n",
    "bestLambdaSVM_bal, lambdas, accuracies_bal, best_lambda_ind_bal = \\\n",
    "chooseBestLambdaSVM(dataArray = balDataArrayFF, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal λ = 989.8990000000001\n",
      "Maximum mean of accuracies = 0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "# Optimal lambda hyperparameter for SVM - Balanced data\n",
    "print('Optimal λ = ' + str(bestLambdaSVM_bal))\n",
    "# Corresponding mean of accuracies\n",
    "print('Maximum mean of accuracies = ' + str(accuracies_bal[best_lambda_ind_bal]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9b3e80940>]"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting mean of accuracies against different lambdas : SVM - Balanced data\n",
    "plt.figure()\n",
    "title = 'Mean of accuracies computed using five-fold cross-validation against hyperparameter $\\lambda$ - Balanced data \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Mean of accuracies')\n",
    "plt.plot(lambdas,accuracies_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 729.6589588140864\n",
      "Iteration is: 2, Cost is: 749.6105160297783\n",
      "Iteration is: 4, Cost is: 652.0789393115103\n",
      "Iteration is: 8, Cost is: 513.080291567229\n",
      "Iteration is: 16, Cost is: 337.45865886478305\n",
      "Iteration is: 32, Cost is: 316.88312247139663\n",
      "Iteration is: 64, Cost is: 270.95437577238783\n",
      "Iteration is: 128, Cost is: 264.5983498250545\n",
      "Iteration is: 256, Cost is: 234.0511697328188\n",
      "Iteration is: 512, Cost is: 279.52731220367286\n",
      "Iteration is: 1024, Cost is: 234.21883153299018\n",
      "Iteration is: 1999, Cost is: 232.61187538655582\n"
     ]
    }
   ],
   "source": [
    "# Training the model with optimal lambda - Balanced data\n",
    "wBest_bal = sgd(X = balDataArray[:,1:32], y = balDataArray[:,32],\\\n",
    "                regul_strength = bestLambdaSVM_bal, print_outcome=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 70  13]\n",
      " [  3 114]]\n"
     ]
    }
   ],
   "source": [
    "# Computing confusion matrix - Balanced data\n",
    "TP_bal, TN_bal, FP_bal, FN_bal, confusionMatrix_bal = getConfusionMatrixSVM(w = wBest_bal, \\\n",
    "                                                                            X_test = testingDataArray[:,1:32], \\\n",
    "                                                        y_test = testingDataArray[:,32])\n",
    "print(confusionMatrix_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing ROC - Balanced data\n",
    "FPR_bal, TPR_bal = ROC_SVM(X_train = balDataArray[:,1:32], y_train = balDataArray[:,32], \\\n",
    "        X_test = testingDataArray[:,1:32], y_test = testingDataArray[:,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing best hyperparameter ROC point (FPR,TPR) - Balanced data\n",
    "        \n",
    "# Confusion matrix\n",
    "bestTP_bal, bestTN_bal, bestFP_bal, bestFN_bal, _ = getConfusionMatrixSVM(w = wBest_bal,\\\n",
    "                                                                          X_test = testingDataArray[:,1:32], \\\n",
    "                                                        y_test = testingDataArray[:,32])\n",
    "\n",
    "bestLbdaFPR_bal = bestFP_bal / (bestFP_bal + bestTN_bal) # False positive rate\n",
    "bestLbdaTPR_bal = bestTP_bal / (bestTP_bal + bestFN_bal) # True positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc9b5911670>"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting ROC - Balanced data\n",
    "plt.figure()\n",
    "title = 'Receiver operating characteristic curve of the SVM model - using balanced data \\n'\n",
    "plt.title(title)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim([-0.01,1.1])\n",
    "plt.ylim([-0.01,1.1])\n",
    "plt.plot([i for i in range(2)], [i for i in range(2)], color = 'green', ls = '--')\n",
    "plt.scatter(FPR_bal, TPR_bal, s=5)\n",
    "plt.scatter(bestLbdaFPR_bal, bestLbdaTPR_bal, s=25, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our SVM model on a balanced data set - that is to say on a data set where both classes are balanced in size - produces a receiver operating characteristic curve where optimal hyperparameters maintain the true positive rate to 1 (since no or little false negatives are predicted with said model). The relationship between the predictors and the true targets may perhaps be understood better by our model since such pairs occur more often in said balanced training dataset. Such a true positive rate can be achieved along with a rather good false positive rate (below 0.15). Notice also that for balanced data, the model performs rather similarly well than the unbalanced dataset, even though it has roughly 3 times less annotated training samples available (800 against 2566). As said previously, $\\textit{having less false negatives for a little bit more of false positives is more acceptable in this medical context}$, and thus one may prefer this balanced training set for producing a better suited model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlwygRTDy1fb"
   },
   "source": [
    "<a name=\"task-3\"></a>\n",
    "\n",
    "# Task 3: Mastery Component [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdYowG2Ny1qx"
   },
   "source": [
    "<a name=\"q31\"></a>\n",
    "\n",
    "## 3.1 Logistic regression and bagging  [^](#outline)\n",
    "\n",
    "Once again, we seek to classify tumour samples that can belong to one of two given classes. We here use logistic regression to perform this task. Namely, we compute the probability of belonging to one of the classes by using the logistic function as follows : $P(y=1) = \\frac{1}{1 + e^{-\\mathbf{x}^T\\mathbf{\\beta}}}$. The vector $\\beta$ is a parameter to be found by maximizing the log-likelihood : $L = \\sum_i^N y^{(i)} \\log h_{\\beta}(\\mathbf{x}^{(i)}) + (1-y^{(i)}) \\log (1 - h_{\\beta}(\\mathbf{x}^{(i)}))$. Once a such $\\beta$ has been found, we compute our predictions by using a threshold $\\tau$ such that if $P(y=1|\\mathbf{x}^{in}) > \\tau$ then we have $\\mathbf{x}^{in} = 1$.\n",
    "\n",
    "### 3.1.1 Training \n",
    "\n",
    "We train this model on the same tumour samples training data set used previously. We approximate a vector $\\beta$ maximizing the log-likelihood using graident descent. The threshold considered here is $\\tau = 0.5$. \n",
    "\n",
    "Once again, we $\\textbf{rely on the weekly coding tasks solutions}$, and further $\\textbf{modify them to suit our current problem}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'B' 'B' ... 'B' 'B' 'B']\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# In the weekly Coding Task solution functions, you may find my custom comments in the form of ### _____ ###\n",
    "\n",
    "# Serializing the classes : 'M' becomes 1 and 'B' becomes 0\n",
    "dataArray = df_tumour.to_numpy()\n",
    "print(dataArray[:,31])\n",
    "serialMap = {'M': 1, 'B': 0}\n",
    "length = len(dataArray[:,31])\n",
    "dataArray[:,31] = [serialMap[dataArray[i,31]] for i in range(length)]\n",
    "print(dataArray[:,31])\n",
    "\n",
    "# Standardization\n",
    "dataArray[:,1:31] = (dataArray[:,1:31] - np.mean(dataArray[:,1:31])) / np.std(dataArray[:,1:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIGHTLY MODIFYING THOSE CT4 SOLUTION FUNCTION ## \n",
    "### All 'slight modifications' proceed from small implementation differences between the CT and the coursework \n",
    "# to be fixed \n",
    "def logistic(x):\n",
    "    x = x.astype(float)\n",
    "    return 1. / (1. + np.exp(-x)) \n",
    "\n",
    "def predict_log(X, beta, beta_0):\n",
    "  y_log = logistic(X.T @ beta + beta_0) \n",
    "  return y_log.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIGHTLY MODIFYING THIS CT4 SOLUTION FUNCTION ## \n",
    "\n",
    "def initialise(d):\n",
    "  \"\"\"    \n",
    "  Argument:\n",
    "  d: size of the beta vector (or number of parameters)\n",
    "  \n",
    "  Returns:\n",
    "  beta: initialised vector of shape (d, 1)\n",
    "  beta_0: initialised scalar (corresponds to the offset)\n",
    "  \"\"\"\n",
    "  \n",
    "  beta = np.zeros(shape=(d, 1), dtype=np.float32)\n",
    "  beta_0 = 0\n",
    "  \n",
    "  assert(beta.shape==(d, 1))\n",
    "  assert(isinstance(beta_0, float) or isinstance(beta_0, int))\n",
    "  \n",
    "  return beta, beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIGHTLY MODIFYING THIS CT4 SOLUTION FUNCTION ## \n",
    "\n",
    "def propagate(X, y, beta, beta_0):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X: data of size (d, n)\n",
    "  y: true label vector of size (1, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "\n",
    "  Returns:\n",
    "  cost: negative log-likelihood cost for logistic regression\n",
    "  dbeta: gradient of the loss with respect to beta\n",
    "  dbeta_0: gradient of the loss with respect to beta_0\n",
    "  \"\"\"\n",
    "  n = X.shape[1]\n",
    "  y_log = predict_log(X, beta, beta_0)\n",
    "\n",
    "  # cost function\n",
    "  cost = - (y * np.log(y_log) + (1-y) * np.log(1 - y_log)).mean()\n",
    "\n",
    "  # derivatives\n",
    "  dbeta = (X * (y_log - y)).mean(axis=1).reshape(-1, 1)\n",
    "  dbeta_0 =  (y_log - y).mean() \n",
    "\n",
    "  assert(dbeta.shape==beta.shape)\n",
    "  assert(type(dbeta_0)==float)\n",
    "  cost = np.squeeze(cost)\n",
    "  assert(cost.shape==())\n",
    "  \n",
    "  # store gradients in a dictionary\n",
    "  grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "  \n",
    "  return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT MODIFYING THIS CT4 SOLUTION FUNCTION ## \n",
    "\n",
    "def optimise(X, y, beta, beta_0, num_iterations=5000, learning_rate=0.005, print_cost=False):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X: data of size (d, n)\n",
    "  y: true label vector of size (1, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "  num_iterations: number of iterations gradient descent shall update the parameters\n",
    "  learning_rate: step size in updating procedure\n",
    "  print_cost: whether to print the cost every 500 iterations or not\n",
    "\n",
    "  Returns:\n",
    "  params: dictionary containing the parameters beta and offset beta_0\n",
    "  grads: dictionary containing the gradients\n",
    "  costs: list of all the costs computed during the optimisation (can be used to plot the learning curve).\n",
    "  \"\"\"\n",
    "  costs = []\n",
    "    \n",
    "  for i in range(num_iterations):\n",
    "\n",
    "      # calculate cost and gradients (hint: use your existing functions)\n",
    "      grads, cost = propagate(X, y, beta, beta_0) \n",
    "      \n",
    "      # retrieve derivatives from grads\n",
    "      dbeta = grads[\"dbeta\"]\n",
    "      dbeta_0 = grads[\"dbeta_0\"]\n",
    "      \n",
    "      # updating procedure\n",
    "      beta = beta - learning_rate * dbeta  \n",
    "      beta_0 = beta_0 - learning_rate * dbeta_0  \n",
    "      \n",
    "      # record the costs\n",
    "      if i % 100 == 0:\n",
    "          costs.append(cost)\n",
    "      \n",
    "      # print the cost every 500 iterations\n",
    "      if print_cost and i % 500 == 0:\n",
    "          print (\"cost after iteration %i: %f\" %(i, cost))\n",
    "  \n",
    "  # save parameters and gradients in dictionary\n",
    "  params = {\"beta\": beta, \"beta_0\": beta_0}\n",
    "  grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "  \n",
    "  return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIGHTLY MODIFYING THIS CT4 SOLUTION FUNCTION ## \n",
    "\n",
    "def predict(X_test, beta, beta_0):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X_test: test data of size (d, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "\n",
    "  Returns:\n",
    "  y_pred: vector containing all binary predictions (0/1) for the examples in X_test\n",
    "  y_log: vector containing the probabilities of belonging to each classes \n",
    "  \"\"\"\n",
    "  n = X_test.shape[1]\n",
    "  y_pred = np.zeros((1,n))\n",
    "  beta = beta.reshape(X_test.shape[0], 1)\n",
    "  \n",
    "  # compute vector y_log predicting the probabilities\n",
    "  y_log = predict_log(X_test, beta, beta_0)\n",
    "  \n",
    "  ### Rounding - Equivalent to having a threshold tau = 0.5 ###\n",
    "  y_pred = y_log.round().reshape(1, -1) \n",
    "  \n",
    "  assert(y_pred.shape==(1, n))\n",
    "  \n",
    "  return y_pred, y_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here use $\\texttt{np.round()}$ which rounds the probabilities to the closest integer. This process is equivalent to setting a threshold $\\tau = 0.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([0.1,0.7,0.5,0.1,0.9])\n",
    "test.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0: 0.693147\n",
      "cost after iteration 500: 0.425759\n",
      "cost after iteration 1000: 0.333783\n",
      "cost after iteration 1500: 0.285486\n",
      "cost after iteration 2000: 0.255836\n",
      "cost after iteration 2500: 0.235765\n",
      "cost after iteration 3000: 0.221255\n",
      "cost after iteration 3500: 0.210259\n",
      "cost after iteration 4000: 0.201628\n",
      "cost after iteration 4500: 0.194667\n"
     ]
    }
   ],
   "source": [
    "# Computing beta on training set \n",
    "initialBeta, initialBeta0 = initialise(dataArray[:,1:31].shape[1])\n",
    "params, grads, costs = optimise(X = dataArray[:,1:31].T, y = dataArray[:,31],\\\n",
    "                                beta = initialBeta, beta_0 = initialBeta0, \\\n",
    "                                num_iterations=5000, \\\n",
    "                                learning_rate=0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyLogisticReg(X_train, y_train, X_test, y_test,  num_iterations=5000, learning_rate=0.005) :\n",
    "    \"\"\"\n",
    "    This function computes the accuracy of the logistic regression model on a testing set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : an pxN-dimensionnal numpy array - the train predictors\n",
    "    y_train : an N-dimensionnal numpy array - the train targets\n",
    "    X_test : an pxN-dimensionnal numpy array - the test predictors\n",
    "    y_test : an N-dimensionnal numpy array - the test targets\n",
    "    num_iterations : an integer - the maximum number of iterations for the gradient descent \n",
    "    learning_rate : a float number - the step size of the gradient descent \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : a float number - the accuracy of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing our parameter\n",
    "    initialBeta, initialBeta0 = initialise(X_train.shape[0])\n",
    "    \n",
    "    # Training \n",
    "    parameters, grads, costs = optimise(X = X_train, y = y_train, \\\n",
    "                                        beta = initialBeta, beta_0 = initialBeta0, \\\n",
    "                                        num_iterations = num_iterations, \\\n",
    "                                        learning_rate = learning_rate)\n",
    "    beta = parameters[\"beta\"]\n",
    "    beta0 = parameters[\"beta_0\"]\n",
    "    \n",
    "    # Predicting\n",
    "    yPredictions, _ = predict(X_test, beta, beta0)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 - np.mean(np.abs(yPredictions - y_test)) * 100 # Taken from CT4\n",
    "    \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the classes : 'M' becomes 1 and 'B' becomes 0\n",
    "testingDataArray = df_tumour_test.to_numpy()\n",
    "serialMap = {'M': 1, 'B': 0}\n",
    "length = len(testingDataArray[:,31])\n",
    "testingDataArray[:,31] = [serialMap[testingDataArray[i,31]] for i in range(length)]\n",
    "\n",
    "# Standardization\n",
    "testingDataArray[:,1:31] = (testingDataArray[:,1:31] - np.mean(testingDataArray[:,1:31])) \\\n",
    "/ np.std(testingDataArray[:,1:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our logistic regression model = 82.5\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on testing data\n",
    "accuracyLR = accuracyLogisticReg(X_train = dataArray[:,1:31].T, y_train = dataArray[:,31], \\\n",
    "          X_test = testingDataArray[:,1:31].T, y_test = testingDataArray[:,31])\n",
    "print('Accuracy of our logistic regression model = ' + str(accuracyLR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Bagging and logistic regression\n",
    "\n",
    "We now use the bagging process to improve our method, that is to say we draw $B$ random samples with replacement from our training dataset and we further train the model on each of said samples. We obtain $B$ vectors $\\beta_b$, which further gives us $B$ probability vectors for each new entry value to predict. We aggregate those probability vectors by computing their average. We then use the same threshold $\\tau = 0.5$ on this average vector to obtain our classification prediction.\n",
    "\n",
    "We use the 5-fold cross-validation process to find the optimal number of bootstrap samples $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyLogisticRegBagging(X_train, y_train, X_test, y_test, B, \\\n",
    "                               num_iterations=5000, learning_rate=0.005) :\n",
    "    \"\"\"\n",
    "    This function computes the accuracy of the logistic regression model on a testing set, using the \n",
    "    bagging process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : an pxN-dimensionnal numpy array - the train predictors\n",
    "    y_train : an N-dimensionnal numpy array - the train targets\n",
    "    X_test : an pxN-dimensionnal numpy array - the test predictors\n",
    "    y_test : an N-dimensionnal numpy array - the test targets\n",
    "    B : an integer - the number of bootstrap samples\n",
    "    num_iterations : an integer - the maximum number of iterations for the gradient descent \n",
    "    learning_rate : a float number - the step size of the gradient descent \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : a float number - the accuracy of the model\n",
    "    \"\"\"\n",
    "    # Retreiving N\n",
    "    N = X_train.shape[1]\n",
    "    # Indices in X_train\n",
    "    training_indices = np.arange(N)\n",
    "    \n",
    "    # Number of elements per sample\n",
    "    sampleSize = N//2 # This could be another hyperparameter to fine tune \n",
    "    \n",
    "    # Vector containing the aggregate of probabilities \n",
    "    meanYlog = np.zeros(X_test.shape[1])\n",
    "    \n",
    "    for _ in range(B) :\n",
    "        \n",
    "        # Sampling - Size of samples is N//2\n",
    "        sample = np.random.choice(training_indices, sampleSize, replace=True) # Taken from CT6\n",
    "        X_sample = X_train[: , sample]\n",
    "        y_sample = y_train[sample] \n",
    "        \n",
    "    \n",
    "        # Initializing our parameter\n",
    "        initialBeta, initialBeta0 = initialise(X_sample.shape[0])\n",
    "\n",
    "        # Training \n",
    "        parameters, grads, costs = optimise(X = X_sample, y = y_sample, \\\n",
    "                                            beta = initialBeta, beta_0 = initialBeta0, \\\n",
    "                                            num_iterations = num_iterations, \\\n",
    "                                            learning_rate = learning_rate)\n",
    "        beta = parameters[\"beta\"]\n",
    "        beta0 = parameters[\"beta_0\"]\n",
    "\n",
    "        # Predicting\n",
    "        _, y_log = predict(X_test, beta, beta0)\n",
    "        \n",
    "        # Aggregating \n",
    "        meanYlog += y_log\n",
    "    \n",
    "    # Discretization\n",
    "    meanYlog = (1/B) * meanYlog\n",
    "    yPredictions = meanYlog.round().reshape(1, -1) \n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 - np.mean(np.abs(yPredictions - y_test)) * 100 # Taken from CT4\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we chose here to set the samples size to $\\texttt{N//2}$. This choice is arbitrary and this hyperparameter could be fine tuned also using a cross-validation process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our logistic regression model with bagging (B=3) = 82.5\n"
     ]
    }
   ],
   "source": [
    "accuracyLRwBagging = accuracyLogisticRegBagging(X_train = dataArray[:,1:31].T, y_train = dataArray[:,31], \\\n",
    "          X_test = testingDataArray[:,1:31].T, y_test = testingDataArray[:,31], B = 3)\n",
    "print('Accuracy of our logistic regression model with bagging (B=3) = ' + str(accuracyLRwBagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiveFoldValidationLR(dataArray, B) :\n",
    "    \"\"\"\n",
    "    This function computes the mean of accuracies of a five-fold cross-validation \n",
    "    of the logistic regression model with bagging.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    B : an integer - the number of bootstrap samples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    meanAccuracy : a float number - the mean of accuracies of the five-fold cross-validation \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = np.zeros(5) # Container for the accuracies \n",
    "\n",
    "    # As seen in coding task - List of five index arrays, each correspond to one of the five folds.\n",
    "    folds_indexes = np.split(np.arange(dataArray.shape[0]-1), 5) # We drop the last one to allow division by 5\n",
    "    \n",
    "    for i in range(5) : # Five fold\n",
    "\n",
    "        # Building validation subset\n",
    "        validationSubset  = dataArray[folds_indexes[i]] \n",
    "        training_indexes = [] # Initializing \n",
    "        # Building training subset\n",
    "        for j in range(5) :\n",
    "            if j != i : \n",
    "                training_indexes += list(folds_indexes[j])\n",
    "        trainingSubset = dataArray[training_indexes]\n",
    "        \n",
    "        \n",
    "        # Training and Validation\n",
    "        # On training subset\n",
    "        # On validation subset\n",
    "        accuracies[i] = accuracyLogisticRegBagging(X_train = trainingSubset[:,1:31].T, \\\n",
    "                                                   y_train = trainingSubset[:,31].T, \\\n",
    "                                                   X_test = validationSubset[:,1:31].T, \\\n",
    "                                                   y_test = validationSubset[:,31].T, \\\n",
    "                                                   B = B)\n",
    "         \n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestBlogisticReg(dataArray, returnArray = False):\n",
    "    \"\"\"\n",
    "    This function computes the optimal hyperparameter B by using a five-fold cross-validation\n",
    "    method for the logistic regression with bagging.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataArray : an Nx(p+2)-dimensionnal numpy array containing both the predictors and the target value\n",
    "    returnArray : boolean value - if set to True the function returns the arrays containing the means \n",
    "        of accuracies and the tested B \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_B : an integer - the optimal hyperparameter\n",
    "    Blist : an M-dimensionnal numpy array containing the tested B\n",
    "    BAcc : an M-dimensionnal numpy array containing the means of accuracies\n",
    "    best_B_ind : an integer - the optimal B index in Blist\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    Blist = np.arange(start = 2, stop = 30, step = 3) # Scanning B\n",
    "    accuracies = np.zeros(Blist.shape[0])\n",
    "    \n",
    "    for i, B in enumerate(Blist) :\n",
    "        # Maximizing mean of accuracies\n",
    "        accuracies[i] = fiveFoldValidationLR(dataArray = dataArray, B = B)\n",
    "        print('Cross-validation for B = ' + str(B) + ' gives a mean of accuracies = '\\\n",
    "                  + str(accuracies[i]))\n",
    "        \n",
    "    best_B_ind = np.argmax(accuracies) # A maximizer of means of accuracies\n",
    "    \n",
    "    if returnArray :\n",
    "        return Blist[best_B_ind], Blist, accuracies, best_B_ind\n",
    "    \n",
    "    return Blist[best_B_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for B = 2 gives a mean of accuracies = 94.11306042884989\n",
      "Cross-validation for B = 5 gives a mean of accuracies = 94.07407407407406\n",
      "Cross-validation for B = 8 gives a mean of accuracies = 94.26900584795322\n",
      "Cross-validation for B = 11 gives a mean of accuracies = 94.19103313840155\n",
      "Cross-validation for B = 14 gives a mean of accuracies = 93.99610136452242\n",
      "Cross-validation for B = 17 gives a mean of accuracies = 94.15204678362572\n",
      "Cross-validation for B = 20 gives a mean of accuracies = 94.15204678362572\n",
      "Cross-validation for B = 23 gives a mean of accuracies = 94.03508771929823\n",
      "Cross-validation for B = 26 gives a mean of accuracies = 93.9961013645224\n",
      "Cross-validation for B = 29 gives a mean of accuracies = 93.95711500974657\n"
     ]
    }
   ],
   "source": [
    "bestB, Blist, accuracies, best_B_ind = chooseBestBlogisticReg(dataArray = dataArray, returnArray = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal B = 8\n",
      "Maximum mean of accuracies = 94.26900584795322\n"
     ]
    }
   ],
   "source": [
    "# Optimal B hyperparameter for logistic regression with bagging\n",
    "print('Optimal B = ' + str(bestB))\n",
    "# Corresponding mean of accuracies\n",
    "print('Maximum mean of accuracies = ' + str(accuracies[best_B_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3\n",
    "\n",
    "We now compare the accuracy obtained by the model with and without bagging, on the testing set, while using our optimal parameter $B=8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our logistic regression model = 82.5\n"
     ]
    }
   ],
   "source": [
    "accuracyLR = accuracyLogisticReg(X_train = dataArray[:,1:31].T, y_train = dataArray[:,31], \\\n",
    "          X_test = testingDataArray[:,1:31].T, y_test = testingDataArray[:,31])\n",
    "print('Accuracy of our logistic regression model = ' + str(accuracyLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our logistic regression model with bagging (B=8) = 83.0\n"
     ]
    }
   ],
   "source": [
    "accuracyLRwBaggingBestB = accuracyLogisticRegBagging(X_train = dataArray[:,1:31].T, y_train = dataArray[:,31], \\\n",
    "          X_test = testingDataArray[:,1:31].T, y_test = testingDataArray[:,31], B = bestB)\n",
    "print('Accuracy of our logistic regression model with bagging (B=8) = ' + str(accuracyLRwBaggingBestB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we compute the predictions using the logistic regression model on the full training set we obtain an accuracy close to 94%. However, the accuracy computed on the unused testing set is rather lower (82.5%). This might highlight an overfitting of the model on the training data. We further use a bagging process, randomly sampling the training data and then aggregating the results in order to reduce the variability arising from any new entry data. Even though the accuracy obtained using bagging is better (83%), applying this method does not improve substantially the performance of our model on unseen data. Thus, one may prefer using one of our better performing previously seen models (SVM, Random forests).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XbuLm6qy100"
   },
   "source": [
    "<a name=\"q32\"></a>\n",
    "\n",
    "## 3.2 Kernelised SVM classifier [^](#outline)\n",
    "\n",
    "We now perform the same classification task, again using a soft margin SVM, this time implementing a kernelised version of the method. Namely, we replace the dot products involved in the optimisation process of the classic SVM method by a non-linear kernel function. The kernel function maps two vectors to the dot product of a pair of nonlinearly transformed vectors of higher-dimension. The idea is to lift our data to a higher dimensional space, in which a separating hyperplane can be found with low penalties. In pratice, we choose those kernel functions such that we only have to compute a function of the usual dot-product. \n",
    "\n",
    "The kernel function we consider here is the Gaussian radial basis function given by $k(\\mathbf{x},\\mathbf{y}) = e^{\\frac{-||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma}}$. \n",
    "\n",
    "In this new setting, we are seeking to minimize : $\\min_{\\mathbf{\\omega_z}} \\frac{1}{2}||\\mathbf{\\omega_z}||^2 + \\frac{\\lambda}{N} \\sum_i^N \\max(0,1-y^{(i)}(\\mathbf{\\omega_z}\\cdot\\mathbf{z}^{(i)}+b))$, where $\\mathbf{\\omega_z}\\cdot\\mathbf{z}^{(i)} = k(\\mathbf{\\omega},\\mathbf{x}^{(i)})$. \n",
    "\n",
    "### 3.2.1 Implementation\n",
    "\n",
    "Notice that $||\\mathbf{\\omega_z}||^2 = \\mathbf{\\omega_z} \\cdot \\mathbf{\\omega_z} = k(\\mathbf{\\omega},\\mathbf{\\omega}) = 1$. We thus seek to minimize : $\\sum_i^N \\max(0,1-y^{(i)}(k(\\mathbf{\\omega},\\mathbf{x}^{(i)})+b))$. The gradient sums either zeros or terms of the form $-\\frac{2}{\\sigma}y^{(i)}k(\\mathbf{\\omega},\\mathbf{x}^{(i)})(\\mathbf{\\omega}-\\mathbf{x}^{(i)}) $.\n",
    "\n",
    "We modify the functions implemented previously, and we build several new helper functions to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the weekly Coding Task solutions functions, you may find my custom comments in the form of ### _____ ###\n",
    "\n",
    "# Serializing the classes : 'M' becomes 1.0 and 'B' becomes -1.0\n",
    "dataArray = df_tumour.to_numpy()\n",
    "serialMap = {'M': 1.0, 'B': -1.0}\n",
    "length = len(dataArray[:,31])\n",
    "dataArray[:,31] = [serialMap[dataArray[i,31]] for i in range(length)]\n",
    "\n",
    "# Standardization\n",
    "dataArray[:,1:31] = (dataArray[:,1:31] - np.mean(dataArray[:,1:31])) / np.std(dataArray[:,1:31])\n",
    "\n",
    "# Augmenting dataArray to fit intercept \n",
    "#dataArray = np.insert(dataArray, 31, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ker(x,y,sigma) :\n",
    "    \"\"\"\n",
    "    This function computes the Gaussian radial basis kernel function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : an M-dimensionnal numpy array - a vector\n",
    "    y : an M-dimensionnal numpy array - a vector\n",
    "    sigma : a float number - a hyperparameter\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ker : a float number - the value of the Gaussian radial basis kernel function\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.exp(-(np.linalg.norm(x-y)**2)/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "def compute_cost(w, X, y, sigma):\n",
    "    n = X.shape[0]\n",
    "    distances = np.zeros(n)\n",
    "    for i in range(n) :\n",
    "        distances[i] = 1 - y[i] * (ker(X[i],w[1:],sigma) + w[0]) ### Using ker ###\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "\n",
    "    # calculate cost\n",
    "    return np.sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "# calculate gradient of cost\n",
    "def calculate_cost_gradient(w, X_batch, y_batch, sigma):\n",
    "    # if only one example is passed\n",
    "    y_batch = np.asarray([y_batch])\n",
    "    X_batch = np.asarray([X_batch])  # gives multidimensional array\n",
    "\n",
    "    #distance = 1 - (y_batch * (X_batch @ w))\n",
    "    n = X_batch.shape[0]\n",
    "    distances = np.zeros(n)\n",
    "    for i in range(n) :\n",
    "        distances[i] = 1 - y_batch[i] * ker(X_batch[i],w[1:],sigma) ### Using ker ###\n",
    "    dw = np.zeros(len(w), dtype = float)\n",
    "\n",
    "    for ind, d in enumerate(distances):\n",
    "        if max(0, d)!=0:\n",
    "            #di = w - (regul_strength * y_batch[ind] * X_batch[ind])\n",
    "            ### Not efficient ###\n",
    "            ### Using ker ###\n",
    "            di = np.zeros(len(w), dtype = float)\n",
    "            di[1:] = -(2/sigma) * y_batch[ind] * ker(X_batch[ind],w[1:],sigma) * (w[1:] - X_batch[ind])\n",
    "            di[0] = -y_batch[ind]\n",
    "            di = di.astype(float)\n",
    "            dw += di\n",
    "\n",
    "    return dw/len(y_batch)  # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "def sgd(X, y, batch_size=32, max_iterations=5000, stop_criterion=1e-20, \\\n",
    "        learning_rate=1e-5, sigma=1, print_outcome=False):\n",
    "    # initialise zero weights\n",
    "    weights = np.zeros(X.shape[1]+1)\n",
    "    #np.random.uniform(size=X.shape[1]+1)\n",
    "    nth = 0\n",
    "    # initialise starting cost as infinity\n",
    "    prev_cost = np.inf\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    indices = np.arange(len(y))\n",
    "    for iteration in range(1, max_iterations):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        np.random.shuffle(indices)\n",
    "        batch_idx = indices[:batch_size]\n",
    "        X_b, y_b = X[batch_idx], y[batch_idx]\n",
    "        for xi, yi in zip(X_b, y_b):\n",
    "            ascent = calculate_cost_gradient(weights, xi, yi, sigma) ### Using sigma ###\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^n'th iteration\n",
    "        if iteration==2**nth or iteration==max_iterations-1:\n",
    "            # compute cost\n",
    "            cost = compute_cost(weights, X, y, sigma) ### Using sigma ###\n",
    "            if print_outcome:\n",
    "                print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "            # stop criterion\n",
    "            if abs(prev_cost - cost) < stop_criterion * prev_cost:\n",
    "                return weights\n",
    "              \n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 2565.55332\n",
      "Iteration is: 2, Cost is: 2565.10664\n",
      "Iteration is: 4, Cost is: 2564.52252\n",
      "Iteration is: 8, Cost is: 2563.1824800000004\n",
      "Iteration is: 16, Cost is: 2560.2962399999997\n",
      "Iteration is: 32, Cost is: 2554.1458000000002\n",
      "Iteration is: 64, Cost is: 2542.257240000001\n",
      "Iteration is: 128, Cost is: 2518.651920000001\n",
      "Iteration is: 256, Cost is: 2472.57515999999\n",
      "Iteration is: 512, Cost is: 2377.6384800000174\n",
      "Iteration is: 1024, Cost is: 2188.349239999864\n",
      "Iteration is: 2048, Cost is: 1808.2245599994835\n",
      "Iteration is: 4096, Cost is: 1056.0154400023453\n",
      "Iteration is: 4999, Cost is: 877.9343999993837\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "w = sgd(X = dataArray[:,1:31], y = dataArray[:,31], sigma=0.000001, print_outcome=True)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFYING THIS CT7 SOLUTION FUNCTION ## \n",
    "\n",
    "### Using accuracy ###\n",
    "def score(w, X, y, sigma):\n",
    "    n = X.shape[0]\n",
    "    y_preds = np.zeros(n)\n",
    "    for i in range(n) :\n",
    "        y_preds[i] = np.sign(ker(X[i],w[1:],sigma) + w[0]) ### Using ker ###\n",
    "    return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8347622759158223"
      ]
     },
     "execution_count": 1317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(w, X = dataArray[:,1:31], y = dataArray[:,31], sigma = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the classes : 'M' becomes 1.0 and 'B' becomes -1.0\n",
    "testingDataArray = df_tumour_test.to_numpy()\n",
    "serialMap = {'M': 1.0, 'B': -1.0}\n",
    "length = len(testingDataArray[:,31])\n",
    "testingDataArray[:,31] = [serialMap[testingDataArray[i,31]] for i in range(length)]\n",
    "\n",
    "# Standardization\n",
    "testingDataArray[:,1:31] = (testingDataArray[:,1:31]\\\n",
    "                            - np.mean(testingDataArray[:,1:31])) / np.std(testingDataArray[:,1:31])\n",
    "\n",
    "# Augmenting dataArray to fit intercept \n",
    "#dataArray = np.insert(dataArray, 31, 1, axis=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SurnameCID_CW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
